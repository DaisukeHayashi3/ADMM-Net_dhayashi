{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "396de9c8-1010-4d06-82a4-979d78dc794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Check the avtive DNN\n",
    "######################\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "GPU_number = 1 # if you wanna use A5000, 2\n",
    "torch.cuda.set_device(GPU_number)\n",
    "print(torch.cuda.get_device_name())\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception('NO GPU!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa3d79d-379a-45b4-a8c8-689ee74c8ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import scipy.io as scio\n",
    "import h5py\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import sys\n",
    "\n",
    "# Create path for Dataset\n",
    "#########################\n",
    "\n",
    "class imgdataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super(Imgdataset, self).__init__()\n",
    "        self.data = []\n",
    "        if os.path.exists(path):\n",
    "            dir_list = os.listdir(path)\n",
    "            dir_list = sorted(dir_list)\n",
    "            self.data = [{'ground_truth': path + '/' + dir_list[i]} for i in range(len(dir_list))]\n",
    "\n",
    "        else:\n",
    "            raise FileNotFoundError('path doesnt exist!')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        ground_truth= self.data[index][\"ground_truth\"]\n",
    "        \n",
    "        with h5py.File(ground_truth, 'r') as f:\n",
    "            gt = torch.from_numpy(np.array(f[\"subframe_ideal128\"])) #32,256,256=c,w,h\n",
    "            depth = torch.from_numpy(np.array(f[\"depth_1024\"]))\n",
    "            \n",
    "        gt = gt.permute(0,1,3,2)#c,h,w\n",
    "        depth = depth.permute(1,0)#c,h,w\n",
    "        return gt, depth\n",
    "        \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "        \n",
    "class Imgdataset_multipath(Dataset):\n",
    "\n",
    "    def __init__(self, path):\n",
    "        super(Imgdataset_multipath, self).__init__()\n",
    "        self.data = []\n",
    "        if os.path.exists(path):\n",
    "            dir_list = os.listdir(path)\n",
    "            dir_list = sorted(dir_list)\n",
    "            self.data = [{'ground_truth': path + '/' + dir_list[i]} for i in range(len(dir_list))]\n",
    "\n",
    "        else:\n",
    "            raise FileNotFoundError('path doesnt exist!')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        ground_truth= self.data[index][\"ground_truth\"]\n",
    "        with h5py.File(ground_truth, 'r') as f:\n",
    "            gt = torch.from_numpy(np.array(f[\"subframe128\"])) #32,256,256=c,w,h\n",
    "            depth = torch.from_numpy(np.array(f[\"depth_1024\"]))\n",
    "            \n",
    "        gt = gt.permute(0,1,3,2)#c,h,w\n",
    "        depth = depth.permute(1,0)#c,h,w\n",
    "        return gt, depth\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# dnn architecture\n",
    "##################\n",
    "\n",
    "class double_conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.d_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.d_conv(x)\n",
    "        return x\n",
    "\n",
    "class Unet(nn.Module):\n",
    "\n",
    "    def __init__(self,in_ch, out_ch):\n",
    "        super(Unet, self).__init__()\n",
    "                \n",
    "        self.dconv_down1 = double_conv(in_ch, 32)\n",
    "        self.dconv_down2 = double_conv(32, 64)\n",
    "        self.dconv_down3 = double_conv(64, 128)       \n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.upsample1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.dconv_up2 = double_conv(64 + 64, 64)\n",
    "        self.dconv_up1 = double_conv(32 + 32, 32)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(32, out_ch, 1)\n",
    "        self.afn_last = nn.Tanh()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        inputs = x\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        \n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.upsample2(conv3)\n",
    "        \n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample1(x)        \n",
    "        x = torch.cat([x, conv1], dim=1)       \n",
    "\n",
    "        x = self.dconv_up1(x)  \n",
    "        x = self.conv_last(x)\n",
    "        x = self.afn_last(x)\n",
    "        \n",
    "        out = x + inputs\n",
    "        \n",
    "        return out\n",
    "\n",
    "class Unet128(nn.Module):\n",
    "\n",
    "    def __init__(self,in_ch, out_ch):\n",
    "        super(Unet128, self).__init__()\n",
    "                \n",
    "        self.dconv_down1 = double_conv(in_ch, 128)\n",
    "        self.dconv_down2 = double_conv(128, 128)\n",
    "        self.dconv_down3 = double_conv(128, 256)       \n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.upsample1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dconv_up2 = double_conv(128 + 128, 128)\n",
    "        self.dconv_up1 = double_conv(128 + 128, 128)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(128, out_ch, 1)\n",
    "        self.afn_last = nn.Tanh()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        inputs = x\n",
    "        conv1 = self.dconv_down1(x)       \n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "       \n",
    "        conv3 = self.dconv_down3(x)     \n",
    "        x = self.upsample2(conv3)      \n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "        \n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample1(x)        \n",
    "        x = torch.cat([x, conv1], dim=1)       \n",
    "\n",
    "        x = self.dconv_up1(x)  \n",
    "        x = self.conv_last(x)\n",
    "        x = self.afn_last(x)\n",
    "        \n",
    "        out = x + inputs\n",
    "        \n",
    "        return out\n",
    "\n",
    "class Unet_depth(nn.Module):\n",
    "\n",
    "    def __init__(self,in_ch, out_ch):\n",
    "        super(Unet_depth, self).__init__()\n",
    "                \n",
    "        self.dconv_down1 = double_conv(in_ch, 32)\n",
    "        self.dconv_down2 = double_conv(32, 64)\n",
    "        self.dconv_down3 = double_conv(64, 128)       \n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.upsample1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dconv_up2 = double_conv(64 + 64, 64)\n",
    "        self.dconv_up1 = double_conv(32 + 32, 32)\n",
    "        \n",
    "        self.conv_last1 = nn.Conv2d(32, 32, 1)\n",
    "        self.afn_last = nn.Tanh()\n",
    "        self.conv_last = nn.Conv2d(32, out_ch, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        inputs = x\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        \n",
    "        conv3 = self.dconv_down3(x)\n",
    "\n",
    "        \n",
    "        x = self.upsample2(conv3)        \n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "        \n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample1(x)        \n",
    "        x = torch.cat([x, conv1], dim=1)       \n",
    "\n",
    "        x = self.dconv_up1(x)  \n",
    "        \n",
    "        x = self.conv_last1(x)\n",
    "        x = self.afn_last(x)\n",
    "        out = x + inputs\n",
    "        out = self.conv_last(out)\n",
    "        return out\n",
    "\n",
    "class Unet_depth128(nn.Module):\n",
    "\n",
    "    def __init__(self,in_ch, out_ch):\n",
    "        super(Unet_depth128, self).__init__()\n",
    "                \n",
    "        self.dconv_down1 = double_conv(in_ch, 128)\n",
    "        self.dconv_down2 = double_conv(128, 128)\n",
    "        self.dconv_down3 = double_conv(128, 256)       \n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.upsample1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dconv_up2 = double_conv(128 + 128, 128)\n",
    "        self.dconv_up1 = double_conv(128 + 128, 128)\n",
    "        \n",
    "        self.conv_last1 = nn.Conv2d(128, 128, 1)\n",
    "        self.afn_last = nn.Tanh()\n",
    "        self.conv_last = nn.Conv2d(128, out_ch, 1)    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        inputs = x\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        \n",
    "        conv3 = self.dconv_down3(x)\n",
    "        \n",
    "        x = self.upsample2(conv3)        \n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "        \n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample1(x)        \n",
    "        x = torch.cat([x, conv1], dim=1)       \n",
    "\n",
    "        x = self.dconv_up1(x)  \n",
    "        \n",
    "        x = self.conv_last1(x)\n",
    "        x = self.afn_last(x)\n",
    "        out = x + inputs\n",
    "        out = self.conv_last(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "# main Network ( 32) \n",
    "##############\n",
    "class ADMM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ADMM, self).__init__()\n",
    "                \n",
    "        self.unet1 = Unet(32, 32)\n",
    "        self.unet2 = Unet(32, 32)\n",
    "        self.unet3 = Unet(32, 32)\n",
    "        self.unet4 = Unet(32, 32)\n",
    "        self.unet5 = Unet(32, 32)\n",
    "        self.unet6 = Unet(32, 32)\n",
    "        self.unet7 = Unet(32, 32)\n",
    "        self.unet8 = Unet(32, 32)\n",
    "        self.unet9 = Unet(32, 32)\n",
    "        self.gamma1 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma2 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma3 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma4 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma5 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma6 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma7 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma8 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma9 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "\n",
    "    def forward(self, y, Phi, Phi_r, Phi_s):\n",
    "        x_list = []\n",
    "        d_list = []\n",
    "        theta = At(y,Phi_r)\n",
    "        b = torch.zeros_like(theta)\n",
    "        ### 1-3\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma1),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet1(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma2),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet2(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma3),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet3(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        ### 4-6\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma4),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet4(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma5),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet5(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma6),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet6(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        ### 7-9\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma7),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet7(x1)\n",
    "        output_depth = torch.argmax(theta,dim = 1)\n",
    "        d_list.append(output_depth)\n",
    "        \n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma8),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet8(x1)\n",
    "        output_depth = torch.argmax(theta,dim = 1)\n",
    "        d_list.append(output_depth)\n",
    "        \n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma9),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet9(x1)\n",
    "        output_depth = torch.argmax(theta,dim = 1)\n",
    "        d_list.append(output_depth)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        x_list.append(d_list[-3])\n",
    "        x_list.append(d_list[-2])\n",
    "        x_list.append(d_list[-1])\n",
    "        \n",
    "        output_list = x_list[-6:]\n",
    "        \n",
    "        return output_list\n",
    "\n",
    "\n",
    "class ADMM_depthnet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ADMM_depthnet, self).__init__()\n",
    "        \n",
    "        self.depthnet =  Unet_depth(32,1)        \n",
    "        self.unet1 = Unet(32, 32)\n",
    "        self.unet2 = Unet(32, 32)\n",
    "        self.unet3 = Unet(32, 32)\n",
    "        self.unet4 = Unet(32, 32)\n",
    "        self.unet5 = Unet(32, 32)\n",
    "        self.unet6 = Unet(32, 32)\n",
    "        self.unet7 = Unet(32, 32)\n",
    "        self.unet8 = Unet(32, 32)\n",
    "        self.unet9 = Unet(32, 32)\n",
    "        self.gamma1 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma2 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma3 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma4 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma5 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma6 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma7 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma8 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma9 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "\n",
    "    def forward(self, y, Phi, Phi_r, Phi_s):\n",
    "        x_list = []\n",
    "        d_list = []\n",
    "        theta = At(y,Phi_r)\n",
    "        b = torch.zeros_like(theta)\n",
    "        ### 1-3\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma1),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet1(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma2),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet2(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma3),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet3(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        ### 4-6\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma4),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet4(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma5),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet5(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma6),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet6(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        ### 7-9\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma7),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet7(x1)\n",
    "        output_depth = self.depthnet(theta)\n",
    "        d_list.append(output_depth)\n",
    "        \n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma8),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet8(x1)\n",
    "        output_depth = self.depthnet(theta)\n",
    "        d_list.append(output_depth)\n",
    "        \n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma9),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet9(x1)\n",
    "        output_depth = self.depthnet(theta)\n",
    "        d_list.append(output_depth)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        x_list.append(d_list[-3])\n",
    "        x_list.append(d_list[-2])\n",
    "        x_list.append(d_list[-1])\n",
    "        \n",
    "        output_list = x_list[-6:]     \n",
    "        return output_list\n",
    "\n",
    "# main Network (128) \n",
    "##############\n",
    "\n",
    "class ADMM_128(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ADMM_128, self).__init__()\n",
    "               \n",
    "        self.unet1 = Unet(128, 128)\n",
    "        self.unet2 = Unet128(128, 128)\n",
    "        self.unet3 = Unet128(128, 128)\n",
    "        self.unet4 = Unet128(128, 128)\n",
    "        self.unet5 = Unet128(128, 128)\n",
    "        self.unet6 = Unet128(128, 128)\n",
    "        self.unet7 = Unet128(128, 128)\n",
    "        self.unet8 = Unet128(128, 128)\n",
    "        self.unet9 = Unet128(128, 128)\n",
    "        self.gamma1 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma2 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma3 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma4 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma5 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma6 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma7 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma8 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma9 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "\n",
    "    def forward(self, y, Phi, Phi_r, Phi_s):\n",
    "        x_list = []\n",
    "        d_list = []\n",
    "        theta = At(y,Phi_r)\n",
    "        b = torch.zeros_like(theta)\n",
    "        ### 1-3\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma1),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet1(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma2),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet2(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma3),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet3(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        ### 4-6\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma4),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet4(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma5),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet5(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma6),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet6(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        ### 7-9\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma7),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet7(x1)\n",
    "        output_depth = torch.argmax(theta,dim = 1)\n",
    "        d_list.append(output_depth)\n",
    "        \n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma8),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet8(x1)\n",
    "        output_depth = torch.argmax(theta,dim = 1)\n",
    "        d_list.append(output_depth)\n",
    "        \n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma9),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet9(x1)\n",
    "        output_depth = torch.argmax(theta,dim = 1)\n",
    "        d_list.append(output_depth)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        x_list.append(d_list[-3])\n",
    "        x_list.append(d_list[-2])\n",
    "        x_list.append(d_list[-1])\n",
    "        \n",
    "        output_list = x_list[-6:]\n",
    "        \n",
    "        return output_list\n",
    "\n",
    "        \n",
    "class ADMM_depthnet128(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ADMM_depthnet128, self).__init__()\n",
    "        \n",
    "        self.depthnet =  Unet_depth128(128,1)        \n",
    "        self.unet1 = Unet128(128, 128)\n",
    "        self.unet2 = Unet128(128, 128)\n",
    "        self.unet3 = Unet128(128, 128)\n",
    "        self.unet4 = Unet128(128, 128)\n",
    "        self.unet5 = Unet128(128, 128)\n",
    "        self.unet6 = Unet128(128, 128)\n",
    "        self.unet7 = Unet128(128, 128)\n",
    "        self.unet8 = Unet128(128, 128)\n",
    "        self.unet9 = Unet128(128, 128)\n",
    "        self.gamma1 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma2 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma3 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma4 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma5 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma6 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma7 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma8 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "        self.gamma9 = torch.nn.Parameter(torch.Tensor([0]))\n",
    "\n",
    "    def forward(self, y, Phi, Phi_r, Phi_s):\n",
    "        x_list = []\n",
    "        d_list = []\n",
    "        theta = At(y,Phi_r)\n",
    "        b = torch.zeros_like(theta)\n",
    "        ### 1-3\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma1),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet1(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma2),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet2(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma3),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet3(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        ### 4-6\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma4),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet4(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma5),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet5(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma6),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet6(x1)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        ### 7-9\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma7),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet7(x1)\n",
    "        output_depth = self.depthnet(theta)\n",
    "        d_list.append(output_depth)\n",
    "        \n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma8),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet8(x1)\n",
    "        output_depth = self.depthnet(theta)\n",
    "        d_list.append(output_depth)\n",
    "        \n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        yb = A(theta+b,Phi_r)\n",
    "        x = theta+b + At(torch.div(y-yb,Phi_s+self.gamma9),Phi_r)\n",
    "        x1 = x-b\n",
    "        theta = self.unet9(x1)\n",
    "        output_depth = self.depthnet(theta)\n",
    "        d_list.append(output_depth)\n",
    "        b = b- (x-theta)\n",
    "        x_list.append(theta)\n",
    "        x_list.append(d_list[-3])\n",
    "        x_list.append(d_list[-2])\n",
    "        x_list.append(d_list[-1])\n",
    "        \n",
    "        output_list = x_list[-6:]\n",
    "        \n",
    "        return output_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48938335-316d-432a-863a-a46bc0d0109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix mulfunction\n",
    "####################\n",
    "\n",
    "def time2file_name(time):\n",
    "    year = time[2:4]\n",
    "    month = time[5:7]\n",
    "    day = time[8:10]\n",
    "    hour = time[11:13]\n",
    "    minute = time[14:16]\n",
    "    time_filename = year + '_' + month + '_' + day + '_' + hour + '_' + minute + '_'\n",
    "    return time_filename\n",
    "\n",
    "def A(xt,Phi_r):\n",
    "    xt =  torch.unsqueeze(xt, 2).repeat(1,1,Phi_r.shape[2],1,1)\n",
    "    yt = xt*Phi_r\n",
    "    yt = torch.sum(yt,1)\n",
    "    return yt\n",
    "\n",
    "def At(y,Phi_r):\n",
    "    temp = torch.unsqueeze(y, 1).repeat(1,Phi_r.shape[1],1,1,1).cuda()\n",
    "    x = temp*Phi_r\n",
    "    x = torch.sum(x,2)\n",
    "    return x/4\n",
    "\n",
    "def A0(x,Phi):\n",
    "    temp = x*Phi\n",
    "    y = torch.sum(temp,1)\n",
    "    return torch.from_numpy(y)\n",
    "\n",
    "def At0(y,Phi):\n",
    "    temp = torch.unsqueeze(y, 1).repeat(1,Phi.shape[1],1,1)\n",
    "    x = temp*Phi\n",
    "    return x\n",
    "\n",
    "# Define train and test\n",
    "#######################\n",
    "\n",
    "criterion  = nn.MSELoss()\n",
    "criterion.cuda()\n",
    "l1_loss = nn.L1Loss()\n",
    "l1_loss.cuda()\n",
    "\n",
    "def test(test_path, result_path, psnr_epoch,mask, mask_r, mask_s, noise_add= True, separate='separate', multi_path=False):\n",
    "    \n",
    "    test_list = os.listdir(test_path)\n",
    "    test_list = sorted(test_list)\n",
    "    psnr_sample = torch.zeros(len(test_list))\n",
    "    pred = []\n",
    "    outdepth = []\n",
    "    \n",
    "    for i in range(len(test_list)):\n",
    "        with h5py.File(test_path + '/' + test_list[i],'r') as f:\n",
    "            if multi_path:\n",
    "                gt = torch.from_numpy(np.array(f[\"subframe128\"]))\n",
    "            else:\n",
    "                gt = torch.from_numpy(np.array(f[\"subframe_ideal128\"]))\n",
    "            depth = torch.from_numpy(np.array(f[\"depth_1024\"]))\n",
    "\n",
    "        gt = torch.unsqueeze(gt.permute(0,1,3,2),0).cuda()#c,h,w\n",
    "        gt = gt.float()\n",
    "        \n",
    "        if separate=='separate':\n",
    "            coef = torch.tensor([1.0991, 1.3081, 0.6170, 1.0767])\n",
    "            coef = torch.unsqueeze(coef,0)\n",
    "            coef = torch.unsqueeze(coef,2)\n",
    "            coef = torch.unsqueeze(coef,3)\n",
    "            coef = torch.unsqueeze(coef,4)\n",
    "            sg = gt.shape\n",
    "            coefg = coef.repeat(sg[0],1,sg[2],sg[3],sg[4]).cuda()\n",
    "            gt = gt*coefg\n",
    "            gt = torch.sum(gt,1) \n",
    "            \n",
    "        elif separate=='noseparate':\n",
    "            a=1.3326\n",
    "            gt = gt*a.cuda()\n",
    "            gt = torch.sum(gt,1) \n",
    "            \n",
    "        elif separate=='normal':\n",
    "            gt = torch.sum(gt,1) \n",
    "        else:\n",
    "            raise Exception('separate error')\n",
    "\n",
    "        depth = torch.unsqueeze(depth.permute(1,0),0).cuda()#c,h,w\n",
    "        depth = depth.float()\n",
    "            \n",
    "        if noise_add:\n",
    "            gt_max = torch.max(gt)\n",
    "            e_elec= 20000/gt_max\n",
    "            noise_poisson =  torch.poisson(gt*e_elec)\n",
    "            noise_gaussian = torch.normal(0,40,size = gt.shape).cuda()\n",
    "            gt = (noise_poisson+noise_gaussian)/e_elec\n",
    "            y = gt.detach()\n",
    "        else:\n",
    "            y=gt.detach()\n",
    "            \n",
    "        y = torch.unsqueeze(y,2)\n",
    "        y=y.repeat([1,1,4,1,1])\n",
    "        Phi = mask.repeat([1,1,1])\n",
    "        Phi_s = mask_s.repeat([1, 1, 128, 128])\n",
    "        Phi_r = mask_r.repeat([1, 1,1, 128, 128])\n",
    "        \n",
    "        y = y*Phi_r\n",
    "        y = torch.sum(y, dim=1)       \n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_pic_list = network(y, Phi, Phi_r, Phi_s)\n",
    "            out_pic = out_pic_list[-4]\n",
    "            out_depth = out_pic_list[-1]\n",
    "            \n",
    "            psnr_1 = 0\n",
    "            for ii in range(frame_num):\n",
    "                out_pic_p = out_pic[0, ii, :, :]\n",
    "                gt_t = gt[0, ii, :, :]\n",
    "                rmse = torch.sqrt(criterion(out_pic_p, gt_t))\n",
    "                rmse = rmse.data\n",
    "                psnr_1 += 10 * torch.log10(1 / criterion(out_pic_p, gt_t))\n",
    "            psnr_1 = psnr_1 / (gt.shape[0] * frame_num)\n",
    "            psnr_sample[i] = psnr_1   \n",
    "            \n",
    "        pred.append(out_pic.cpu().numpy())\n",
    "        outdepth.append(out_depth.cpu().numpy())\n",
    "        \n",
    "    psnr_epoch.append(psnr_sample)\n",
    "    return pred,outdepth, psnr_epoch\n",
    "\n",
    "def train(epoch, learning_rate,mask, mask_r, mask_s,noise_add= True, separate='separate',loss_alpha=0.5,loss_beta_i=0.5,loss_beta_d=0.5,multi_path=False):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    begin = time.time()\n",
    "    if epoch<=500:\n",
    "        optimizer = optim.Adam([{\"params\": network.depthnet.parameters(),\"lr\": depth_learning_rate*1},\n",
    "                                  {'params': network.unet1.parameters()},\n",
    "                                  {'params': network.unet2.parameters()},\n",
    "                                  {'params': network.unet3.parameters()},\n",
    "                                  {'params': network.unet4.parameters()},\n",
    "                                  {'params': network.unet5.parameters()},\n",
    "                                  {'params': network.unet6.parameters()},\n",
    "                                  {'params': network.unet7.parameters()},\n",
    "                                  {'params': network.unet8.parameters()},\n",
    "                                  {'params': network.unet9.parameters()},\n",
    "                                 ], lr=learning_rate*1)\n",
    "    else:\n",
    "        optimizer = optim.Adam([{\"params\": network.depthnet.parameters(),\"lr\": depth_learning_rate*1},\n",
    "                                  {'params': network.unet1.parameters()},\n",
    "                                  {'params': network.unet2.parameters()},\n",
    "                                  {'params': network.unet3.parameters()},\n",
    "                                  {'params': network.unet4.parameters()},\n",
    "                                  {'params': network.unet5.parameters()},\n",
    "                                  {'params': network.unet6.parameters()},\n",
    "                                  {'params': network.unet7.parameters()},\n",
    "                                  {'params': network.unet8.parameters()},\n",
    "                                  {'params': network.unet9.parameters()},\n",
    "                                 ], lr=learning_rate)\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "\n",
    "        \n",
    "        for iteration, batch in enumerate(train_data_loader):\n",
    "            gt, depth = Variable(batch[0]), Variable(batch[1])\n",
    "            gt = gt.cuda() # [batch,32,256,256]\n",
    "            gt = gt.float()\n",
    "            \n",
    "            if separate=='separate':\n",
    "                coef= torch.rand(4)+0.5\n",
    "                coef = torch.unsqueeze(coef,0)\n",
    "                coef = torch.unsqueeze(coef,2)\n",
    "                coef = torch.unsqueeze(coef,3)\n",
    "                coef = torch.unsqueeze(coef,4)\n",
    "                sg = gt.shape\n",
    "                coefg = coef.repeat(sg[0],1,sg[2],sg[3],sg[4]).cuda()\n",
    "                gt = torch.sum(gt,1) \n",
    "            elif separate=='noseparate':\n",
    "                a = torch.rand(1)+0.5\n",
    "                gt = gt*a.cuda()\n",
    "                gt = torch.sum(gt,1) \n",
    "            elif separate=='normal':\n",
    "                gt = torch.sum(gt,1) \n",
    "            else:\n",
    "                raise Exception('separate error')\n",
    "                      \n",
    "            depth = torch.unsqueeze(depth,1).cuda()\n",
    "            depth = depth.float()\n",
    "            batch_size1 = gt.shape[0]\n",
    "\n",
    "            if noise_add:\n",
    "                gt_max = torch.max(gt)\n",
    "                e_elec= 20000/gt_max\n",
    "                noise_poisson =  torch.poisson(gt*e_elec)\n",
    "                noise_gaussian = torch.normal(0,40,size = gt.shape)\n",
    "                gt = (noise_poisson+noise_gaussian)/e_elec\n",
    "                y0 = gt.detach()\n",
    "            else:\n",
    "                y0=gt.detach()\n",
    "                \n",
    "            y0 = torch.unsqueeze(y0,2)\n",
    "            y0=y0.repeat([1,1,4,1,1])\n",
    "            \n",
    "            Phi_r = mask_r.repeat([batch_size1, 1,1, 128, 128])\n",
    "            Phi = mask.repeat([batch_size1,1,1])           \n",
    "            Phi_s = torch.sum(Phi_r,dim=1) \n",
    "            \n",
    "            y = y0*Phi_r\n",
    "            y = torch.sum(y, dim=1)\n",
    "            optimizer.zero_grad()\n",
    "            model_out = network(y, Phi, Phi_r, Phi_s)\n",
    "\n",
    "            Loss1 = torch.sqrt(criterion(model_out[-4], gt)) + 0.5*torch.sqrt(criterion(model_out[-5], gt)) + 0.5*torch.sqrt(criterion(model_out[-6], gt))\n",
    "            Loss2 = torch.sqrt(criterion(model_out[-1],depth))+0.5*torch.sqrt(criterion(model_out[-2],depth))+0.5*torch.sqrt(criterion(model_out[-3],depth))\n",
    "            Loss3 = l1_loss(model_out[-4], gt) + 0.5*l1_loss(model_out[-5], gt) + 0.5*l1_loss(model_out[-6], gt)\n",
    "            Loss4 = l1_loss(model_out[-1],depth)+0.5*l1_loss(model_out[-2],depth)+0.5*l1_loss(model_out[-3],depth)\n",
    "            Loss = Loss1*(1-loss_alpha)+Loss3*(1-loss_alpha)+Loss2*loss_alpha\n",
    "            \n",
    "            epoch_loss += Loss.data\n",
    "\n",
    "            Loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    end = time.time()\n",
    "    print(\"===> Epoch {} Complete: Avg. Loss: {:.7f}\".format(epoch, epoch_loss / len(train_data_loader)), \"  time: {:.2f}\".format(end - begin))\n",
    "    return (epoch_loss / len(train_data_loader),mask,mask_r,mask_s)\n",
    "\n",
    "def checkpoint(epoch, model_path,mask,mask_r):\n",
    "    model_out_path = './' + model_path + '/S{}'.format(stage_num) + \"_model_epoch_{}.pth\".format(epoch)\n",
    "    model_out_path2 = './' + model_path + '/S{}'.format(stage_num) + \"_model_epoch_{}_state_dict.pth\".format(epoch)\n",
    "    torch.save(network, model_out_path)\n",
    "    torch.save(network.state_dict(), model_out_path2)\n",
    "    print(\"Checkpoint saved to {}\".format(model_out_path))\n",
    "\n",
    "def main(learning_rate,depth_learning_rate,model_name,mask, mask_r, mask_s,noise_add= True, separate='separate',loss_alpha=0.5,loss_beta_i=0.5,loss_beta_d=0.5,multi_path=False):\n",
    "    date_time = str(datetime.datetime.now())\n",
    "    date_time = time2file_name(date_time)\n",
    "    \n",
    "    result_name = date_time +model_name\n",
    "    result_path = 'recon' + '/' + result_name\n",
    "    model_path = 'model' + '/' + date_time +model_name\n",
    "    \n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "        \n",
    "    file_para = open(model_path+r'/para.txt','w' )\n",
    "    file_para.write('model_name'+':'+model_name+'\\n')\n",
    "    file_para.write('learning_rate'+':'+str(learning_rate)+'\\n')\n",
    "    file_para.write('depth_learning_rate'+':'+str(depth_learning_rate)+'\\n')\n",
    "    file_para.write('noise_add'+':'+str(noise_add)+'\\n')\n",
    "    file_para.write('separate'+':'+separate+'\\n')\n",
    "    file_para.write('loss_alpha'+':'+str(loss_alpha)+'\\n')\n",
    "    file_para.write('loss_beta_i'+':'+str(loss_beta_i)+'\\n')\n",
    "    file_para.write('loss_beta_d'+':'+str(loss_beta_d)+'\\n')\n",
    "    file_para.write('multi_path'+':'+str(multi_path)+'\\n')\n",
    "    file_para.close()\n",
    "    \n",
    "    print('model_name'+':'+model_name)\n",
    "    print('learning_rate'+':{}'.format(learning_rate))\n",
    "    print('depth_learning_rate'+':{}'.format(depth_learning_rate))\n",
    "    print('noise_add'+':{}'.format(noise_add))\n",
    "    print('separate'+':'+separate)\n",
    "    print('loss_alpha'+':{}'.format(loss_alpha))\n",
    "    print('loss_beta_i'+':{}'.format(loss_beta_i))\n",
    "    print('loss_beta_d'+':{}'.format(loss_beta_d))\n",
    "    print('multi_path'+':{}'.format(multi_path))\n",
    "    \n",
    "    psnr_epoch = []\n",
    "    psnr_epoch2 = []\n",
    "    loss_log = []\n",
    "    psnr_max = 0\n",
    "    checkpoint(0, model_path,mask,mask_r)\n",
    "\n",
    "    for epoch in range(last_epoch + 1, last_epoch + max_iter + 1):\n",
    "        train_out = train(epoch, learning_rate,mask, mask_r, mask_s,noise_add,separate,loss_alpha,loss_beta_i,loss_beta_d,multi_path)\n",
    "        epoch_loss = train_out[0].detach()\n",
    "        mask = train_out[1].cuda().detach()\n",
    "        mask_r = train_out[2].cuda().detach()\n",
    "        mask_s = train_out[3].cuda().detach()\n",
    "        loss_log.append(epoch_loss)\n",
    "        loss_log_n = torch.tensor(loss_log).to('cpu').detach().numpy().copy()\n",
    "        np.save(r'./' + model_path+r'/loss_log',loss_log_n)\n",
    "        np.savetxt(r'./' + model_path+r'/loss_log.txt',loss_log_n)\n",
    "        \n",
    "        if (epoch % 50 == 0) and (epoch < 500):\n",
    "            learning_rate = learning_rate * 0.9\n",
    "            depth_learning_rate = depth_learning_rate*0.9\n",
    "\n",
    "        if epoch%100 == 0 or epoch==1:\n",
    "            pred,outdepth, psnr_epoch = test(test_path1,  result_path, psnr_epoch,mask, mask_r, mask_s,noise_add,'separate',multi_path)\n",
    "            print(psnr_epoch)\n",
    "            psnr_mean = torch.mean(psnr_epoch[-1])\n",
    "            print(\"Test result: {:.4f}\".format(psnr_mean))\n",
    "            checkpoint(epoch, model_path,mask,mask_r)\n",
    "            name = result_path + '/S{}'.format(stage_num) + '_pred_' + '{}_{:.4f}'.format(epoch, psnr_mean) + '.mat'\n",
    "            scio.savemat(name, {'pred': pred})\n",
    "            name2 = result_path + '/S{}'.format(stage_num) + '_outdepth_' + '{}_{:.4f}'.format(epoch, psnr_mean) + '.mat'\n",
    "            scio.savemat(name2, {'outdepth': outdepth})\n",
    "            \n",
    "        if math.isnan(epoch_loss):   \n",
    "            break            \n",
    "\n",
    "def generate_masks2(mask_path,shutter_bit,init_mask = 'real_opt_4x'):\n",
    "    if init_mask == 'real_opt_4x':\n",
    "        with h5py.File(mask_path + '/MAU19_optimized_shutter_606MHz_2Bit_4samp.mat', 'r') as f:\n",
    "            mask =np.array(f[\"a\"]) \n",
    "\n",
    "    over_rate = int(frame_num/shutter_bit)\n",
    "    mask_r0 = np.repeat(mask,over_rate,axis = 0)    \n",
    "    mask_r = np.zeros((frame_num, 4, 2, 2))\n",
    "    p=0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            for k in range(4):\n",
    "                mask_r[:,k,j,i] = mask_r0[:,p]\n",
    "                p = p+1\n",
    "\n",
    "    mask_s = np.sum(mask_r, axis=0)\n",
    "    index = np.where(mask_s == 0)\n",
    "    mask_s[index] = 1\n",
    "    \n",
    "    mask = mask.reshape([shutter_bit,4,4])\n",
    "    mask = torch.from_numpy(mask)\n",
    "    mask = mask.float()\n",
    "    mask = mask.cuda()\n",
    "    \n",
    "    mask_s = torch.from_numpy(mask_s)\n",
    "    mask_s = mask_s.float()\n",
    "    mask_s = mask_s.cuda()\n",
    "    \n",
    "    mask_r = torch.from_numpy(mask_r)\n",
    "    mask_r = mask_r.float()\n",
    "    mask_r = mask_r.cuda()\n",
    "    \n",
    "    return mask, mask_r, mask_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e83f3ed-3d85-46f0-b0dd-2dce14d19c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "     # TRAIN \n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "750f4a07-fda6-4cfb-bd17-1ad09ecbced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparam\n",
    "###################\n",
    "\n",
    "# to use checkpoint\n",
    "last_epoch=0\n",
    "model_save_filename = ''\n",
    "\n",
    "# traing param\n",
    "max_iter = 1100\n",
    "noise_add=False # out of services\n",
    "batch_size =4\n",
    "stage_num = 9\n",
    "mode = 'train'  # train or test\n",
    "learning_rate = 0.002\n",
    "depth_learning_rate = 0.002\n",
    "loss_alpha = 0.5 #the ratio of the enhancement reproduction and depth estimation.\n",
    "loss_beta_i = 201\n",
    "loss_beta_d = 200\n",
    "\n",
    "# to change shutter patturns and datasets\n",
    "data_path = r\"./dataset/train128\"  \n",
    "test_path1 = r\"./dataset/test128\"  # simulation data for comparison\n",
    "tap_num = 4\n",
    "mask_path = r\"./matlab\"\n",
    "init_mask = 'real_opt_4x'\n",
    "separate='separate' #'separate' , 'noseparate' ,'normal'\n",
    "frame_num = 128 #the lenght of transient images.\n",
    "shutter_bit = frame_num\n",
    "mask, mask_r, mask_s = generate_masks2(mask_path,shutter_bit,init_mask)\n",
    "\n",
    "multi_path = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb14a6e-0015-4174-a713-e0219e5a159f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:Recons_depth_4x(pretrain)\n",
      "learning_rate:0.002\n",
      "depth_learning_rate:0.002\n",
      "noise_add:False\n",
      "separate:separate\n",
      "loss_alpha:0.5\n",
      "loss_beta_i:201\n",
      "loss_beta_d:200\n",
      "multi_path:True\n",
      "Checkpoint saved to ./model/25_02_09_16_45_Recons_depth_4x(pretrain)/S9_model_epoch_0.pth\n",
      "===> Epoch 1 Complete: Avg. Loss: 0.8492954   time: 146.58\n",
      "[tensor([19.1600, 19.4980, 10.8066, 17.1689, 17.4198, 14.3987, 10.3160, 13.0090,\n",
      "        13.6839, 18.7218, 18.2228, 16.7289, 19.3871, 18.6625, 18.2957])]\n",
      "Test result: 16.3653\n",
      "Checkpoint saved to ./model/25_02_09_16_45_Recons_depth_4x(pretrain)/S9_model_epoch_1.pth\n",
      "===> Epoch 2 Complete: Avg. Loss: 0.6977388   time: 143.01\n",
      "===> Epoch 3 Complete: Avg. Loss: 0.5834948   time: 142.97\n",
      "===> Epoch 4 Complete: Avg. Loss: 0.5928745   time: 143.12\n",
      "===> Epoch 5 Complete: Avg. Loss: 0.5447168   time: 143.21\n",
      "===> Epoch 6 Complete: Avg. Loss: 0.6296815   time: 142.99\n",
      "===> Epoch 7 Complete: Avg. Loss: 0.5236457   time: 142.94\n",
      "===> Epoch 8 Complete: Avg. Loss: 0.4972136   time: 143.36\n",
      "===> Epoch 9 Complete: Avg. Loss: 0.5066218   time: 143.02\n",
      "===> Epoch 10 Complete: Avg. Loss: 0.5606379   time: 142.67\n",
      "===> Epoch 11 Complete: Avg. Loss: 0.4840115   time: 143.37\n",
      "===> Epoch 12 Complete: Avg. Loss: 0.4617989   time: 143.37\n",
      "===> Epoch 13 Complete: Avg. Loss: 0.4505048   time: 143.20\n",
      "===> Epoch 14 Complete: Avg. Loss: 0.4684379   time: 142.49\n",
      "===> Epoch 15 Complete: Avg. Loss: 0.4296660   time: 142.06\n",
      "===> Epoch 16 Complete: Avg. Loss: 0.4659894   time: 142.12\n",
      "===> Epoch 17 Complete: Avg. Loss: 0.4604520   time: 142.34\n",
      "===> Epoch 18 Complete: Avg. Loss: 0.4250473   time: 142.05\n",
      "===> Epoch 19 Complete: Avg. Loss: 0.4340598   time: 142.30\n",
      "===> Epoch 20 Complete: Avg. Loss: 0.4175426   time: 142.19\n",
      "===> Epoch 21 Complete: Avg. Loss: 0.4351043   time: 141.88\n",
      "===> Epoch 22 Complete: Avg. Loss: 0.4062750   time: 142.31\n",
      "===> Epoch 23 Complete: Avg. Loss: 0.4306378   time: 142.26\n",
      "===> Epoch 24 Complete: Avg. Loss: 0.4349216   time: 142.05\n",
      "===> Epoch 25 Complete: Avg. Loss: 0.4227319   time: 142.15\n",
      "===> Epoch 26 Complete: Avg. Loss: 0.3856294   time: 142.29\n",
      "===> Epoch 27 Complete: Avg. Loss: 0.4736650   time: 142.54\n",
      "===> Epoch 28 Complete: Avg. Loss: 0.4189733   time: 141.96\n",
      "===> Epoch 29 Complete: Avg. Loss: 0.3849578   time: 142.08\n",
      "===> Epoch 30 Complete: Avg. Loss: 0.4181257   time: 141.81\n",
      "===> Epoch 31 Complete: Avg. Loss: 0.4080747   time: 142.03\n",
      "===> Epoch 32 Complete: Avg. Loss: 0.4139841   time: 141.89\n",
      "===> Epoch 33 Complete: Avg. Loss: 0.4571430   time: 142.43\n",
      "===> Epoch 34 Complete: Avg. Loss: 0.3890568   time: 142.87\n",
      "===> Epoch 35 Complete: Avg. Loss: 0.3979067   time: 143.22\n",
      "===> Epoch 36 Complete: Avg. Loss: 0.3750559   time: 142.60\n",
      "===> Epoch 37 Complete: Avg. Loss: 0.3780602   time: 142.81\n",
      "===> Epoch 38 Complete: Avg. Loss: 0.4021249   time: 142.78\n",
      "===> Epoch 39 Complete: Avg. Loss: 0.3975488   time: 143.00\n",
      "===> Epoch 40 Complete: Avg. Loss: 0.3723568   time: 142.90\n",
      "===> Epoch 41 Complete: Avg. Loss: 0.3952937   time: 142.86\n",
      "===> Epoch 42 Complete: Avg. Loss: 0.3979528   time: 143.61\n",
      "===> Epoch 43 Complete: Avg. Loss: 0.4298045   time: 142.98\n",
      "===> Epoch 44 Complete: Avg. Loss: 0.3932671   time: 143.43\n",
      "===> Epoch 45 Complete: Avg. Loss: 0.3957250   time: 143.42\n",
      "===> Epoch 46 Complete: Avg. Loss: 0.3549666   time: 142.93\n",
      "===> Epoch 47 Complete: Avg. Loss: 0.3867140   time: 143.13\n",
      "===> Epoch 48 Complete: Avg. Loss: 0.3974503   time: 143.11\n",
      "===> Epoch 49 Complete: Avg. Loss: 0.3847293   time: 142.85\n",
      "===> Epoch 50 Complete: Avg. Loss: 0.3664120   time: 143.13\n",
      "===> Epoch 51 Complete: Avg. Loss: 0.3566042   time: 143.12\n",
      "===> Epoch 52 Complete: Avg. Loss: 0.3728191   time: 143.29\n",
      "===> Epoch 53 Complete: Avg. Loss: 0.3671630   time: 142.73\n",
      "===> Epoch 54 Complete: Avg. Loss: 0.3801853   time: 142.88\n",
      "===> Epoch 55 Complete: Avg. Loss: 0.3882404   time: 143.12\n",
      "===> Epoch 56 Complete: Avg. Loss: 0.3633732   time: 143.06\n",
      "===> Epoch 57 Complete: Avg. Loss: 0.3685296   time: 143.18\n",
      "===> Epoch 58 Complete: Avg. Loss: 0.3534434   time: 142.62\n",
      "===> Epoch 59 Complete: Avg. Loss: 0.3794376   time: 143.08\n",
      "===> Epoch 60 Complete: Avg. Loss: 0.3892730   time: 143.37\n",
      "===> Epoch 61 Complete: Avg. Loss: 0.3952088   time: 143.06\n",
      "===> Epoch 62 Complete: Avg. Loss: 0.3827590   time: 143.00\n",
      "===> Epoch 63 Complete: Avg. Loss: 0.3678569   time: 143.07\n",
      "===> Epoch 64 Complete: Avg. Loss: 0.3579047   time: 143.23\n",
      "===> Epoch 65 Complete: Avg. Loss: 0.3458279   time: 142.85\n",
      "===> Epoch 66 Complete: Avg. Loss: 0.3447111   time: 143.33\n",
      "===> Epoch 67 Complete: Avg. Loss: 0.3951139   time: 143.09\n",
      "===> Epoch 68 Complete: Avg. Loss: 0.3643315   time: 142.76\n",
      "===> Epoch 69 Complete: Avg. Loss: 0.3582141   time: 142.95\n",
      "===> Epoch 70 Complete: Avg. Loss: 0.4101610   time: 142.78\n",
      "===> Epoch 71 Complete: Avg. Loss: 0.3719544   time: 142.73\n",
      "===> Epoch 72 Complete: Avg. Loss: 0.3595445   time: 143.15\n",
      "===> Epoch 73 Complete: Avg. Loss: 0.3580340   time: 142.46\n",
      "===> Epoch 74 Complete: Avg. Loss: 0.3443070   time: 142.95\n",
      "===> Epoch 75 Complete: Avg. Loss: 0.3591070   time: 143.11\n",
      "===> Epoch 76 Complete: Avg. Loss: 0.3330389   time: 142.73\n",
      "===> Epoch 77 Complete: Avg. Loss: 0.3242147   time: 142.74\n",
      "===> Epoch 78 Complete: Avg. Loss: 0.3391624   time: 142.73\n",
      "===> Epoch 79 Complete: Avg. Loss: 0.3693908   time: 143.32\n",
      "===> Epoch 80 Complete: Avg. Loss: 0.3481706   time: 143.46\n",
      "===> Epoch 81 Complete: Avg. Loss: 0.3353837   time: 143.26\n",
      "===> Epoch 82 Complete: Avg. Loss: 0.3397681   time: 143.50\n",
      "===> Epoch 83 Complete: Avg. Loss: 0.3290427   time: 142.89\n",
      "===> Epoch 84 Complete: Avg. Loss: 0.3368242   time: 143.25\n",
      "===> Epoch 85 Complete: Avg. Loss: 0.3387673   time: 142.84\n",
      "===> Epoch 86 Complete: Avg. Loss: 0.3614254   time: 142.98\n",
      "===> Epoch 87 Complete: Avg. Loss: 0.3436224   time: 143.60\n",
      "===> Epoch 88 Complete: Avg. Loss: 0.3315271   time: 143.06\n",
      "===> Epoch 89 Complete: Avg. Loss: 0.3304908   time: 143.00\n",
      "===> Epoch 90 Complete: Avg. Loss: 0.3194690   time: 143.16\n",
      "===> Epoch 91 Complete: Avg. Loss: 0.3081108   time: 142.44\n",
      "===> Epoch 92 Complete: Avg. Loss: 0.3127671   time: 142.87\n",
      "===> Epoch 93 Complete: Avg. Loss: 0.3385864   time: 143.25\n",
      "===> Epoch 94 Complete: Avg. Loss: 0.4014626   time: 143.37\n",
      "===> Epoch 95 Complete: Avg. Loss: 0.3280362   time: 143.02\n",
      "===> Epoch 96 Complete: Avg. Loss: 0.3418096   time: 143.21\n",
      "===> Epoch 97 Complete: Avg. Loss: 0.3436206   time: 143.20\n",
      "===> Epoch 98 Complete: Avg. Loss: 0.3179041   time: 143.25\n",
      "===> Epoch 99 Complete: Avg. Loss: 0.3260630   time: 142.48\n",
      "===> Epoch 100 Complete: Avg. Loss: 0.3293647   time: 142.76\n",
      "[tensor([19.1600, 19.4980, 10.8066, 17.1689, 17.4198, 14.3987, 10.3160, 13.0090,\n",
      "        13.6839, 18.7218, 18.2228, 16.7289, 19.3871, 18.6625, 18.2957]), tensor([25.3727, 25.9443, 13.7800, 23.3723, 23.2483, 18.2583, 14.3142, 16.4494,\n",
      "        17.3235, 25.3983, 23.2074, 21.9740, 25.6248, 23.5236, 22.6558])]\n",
      "Test result: 21.3631\n",
      "Checkpoint saved to ./model/25_02_09_16_45_Recons_depth_4x(pretrain)/S9_model_epoch_100.pth\n",
      "===> Epoch 101 Complete: Avg. Loss: 0.3663126   time: 142.77\n",
      "===> Epoch 102 Complete: Avg. Loss: 0.3079138   time: 143.04\n",
      "===> Epoch 103 Complete: Avg. Loss: 0.3063834   time: 142.96\n",
      "===> Epoch 104 Complete: Avg. Loss: 0.3095024   time: 142.61\n",
      "===> Epoch 105 Complete: Avg. Loss: 0.3155782   time: 142.78\n",
      "===> Epoch 106 Complete: Avg. Loss: 0.3106597   time: 142.65\n",
      "===> Epoch 107 Complete: Avg. Loss: 0.3283702   time: 142.52\n",
      "===> Epoch 108 Complete: Avg. Loss: 0.3169417   time: 142.68\n",
      "===> Epoch 109 Complete: Avg. Loss: 0.3007838   time: 142.46\n",
      "===> Epoch 110 Complete: Avg. Loss: 0.3014674   time: 142.93\n",
      "===> Epoch 111 Complete: Avg. Loss: 0.3110614   time: 142.85\n",
      "===> Epoch 112 Complete: Avg. Loss: 0.3029121   time: 142.42\n",
      "===> Epoch 113 Complete: Avg. Loss: 0.2930053   time: 142.83\n",
      "===> Epoch 114 Complete: Avg. Loss: 0.3254060   time: 143.10\n",
      "===> Epoch 115 Complete: Avg. Loss: 0.2913632   time: 143.38\n",
      "===> Epoch 116 Complete: Avg. Loss: 0.2944344   time: 143.02\n",
      "===> Epoch 117 Complete: Avg. Loss: 0.2980897   time: 142.71\n",
      "===> Epoch 118 Complete: Avg. Loss: 0.2795908   time: 142.97\n",
      "===> Epoch 119 Complete: Avg. Loss: 0.3069461   time: 142.50\n",
      "===> Epoch 120 Complete: Avg. Loss: 0.2949504   time: 143.03\n",
      "===> Epoch 121 Complete: Avg. Loss: 0.3030112   time: 142.99\n",
      "===> Epoch 122 Complete: Avg. Loss: 0.2793755   time: 143.15\n",
      "===> Epoch 123 Complete: Avg. Loss: 0.2809080   time: 143.45\n",
      "===> Epoch 124 Complete: Avg. Loss: 0.2778368   time: 143.14\n",
      "===> Epoch 125 Complete: Avg. Loss: 0.2961370   time: 143.01\n",
      "===> Epoch 126 Complete: Avg. Loss: 0.2827150   time: 142.94\n",
      "===> Epoch 127 Complete: Avg. Loss: 0.2992290   time: 142.94\n",
      "===> Epoch 128 Complete: Avg. Loss: 0.2748797   time: 142.69\n",
      "===> Epoch 129 Complete: Avg. Loss: 0.2773705   time: 143.40\n",
      "===> Epoch 130 Complete: Avg. Loss: 0.2743743   time: 142.87\n",
      "===> Epoch 131 Complete: Avg. Loss: 0.2829250   time: 143.34\n",
      "===> Epoch 132 Complete: Avg. Loss: 0.2963464   time: 143.25\n",
      "===> Epoch 133 Complete: Avg. Loss: 0.2735763   time: 142.92\n",
      "===> Epoch 134 Complete: Avg. Loss: 0.2676129   time: 143.24\n",
      "===> Epoch 135 Complete: Avg. Loss: 0.2670762   time: 142.90\n",
      "===> Epoch 136 Complete: Avg. Loss: 0.2822562   time: 143.05\n",
      "===> Epoch 137 Complete: Avg. Loss: 0.2893731   time: 143.15\n",
      "===> Epoch 138 Complete: Avg. Loss: 0.3425235   time: 143.12\n",
      "===> Epoch 139 Complete: Avg. Loss: 0.2701888   time: 142.77\n",
      "===> Epoch 140 Complete: Avg. Loss: 0.2672601   time: 142.66\n",
      "===> Epoch 141 Complete: Avg. Loss: 0.2833637   time: 142.53\n",
      "===> Epoch 142 Complete: Avg. Loss: 0.2515561   time: 142.48\n",
      "===> Epoch 143 Complete: Avg. Loss: 0.2578778   time: 143.00\n",
      "===> Epoch 144 Complete: Avg. Loss: 0.2656980   time: 143.10\n",
      "===> Epoch 145 Complete: Avg. Loss: 0.2692134   time: 142.65\n",
      "===> Epoch 146 Complete: Avg. Loss: 0.2927875   time: 142.83\n",
      "===> Epoch 147 Complete: Avg. Loss: 0.2715472   time: 142.76\n",
      "===> Epoch 148 Complete: Avg. Loss: 0.2824388   time: 143.28\n",
      "===> Epoch 149 Complete: Avg. Loss: 0.2570123   time: 143.03\n",
      "===> Epoch 150 Complete: Avg. Loss: 0.2624713   time: 142.84\n",
      "===> Epoch 151 Complete: Avg. Loss: 0.2519147   time: 142.96\n",
      "===> Epoch 152 Complete: Avg. Loss: 0.2563136   time: 143.38\n",
      "===> Epoch 153 Complete: Avg. Loss: 0.3179176   time: 142.83\n",
      "===> Epoch 154 Complete: Avg. Loss: 0.2701682   time: 142.82\n",
      "===> Epoch 155 Complete: Avg. Loss: 0.2597306   time: 143.30\n",
      "===> Epoch 156 Complete: Avg. Loss: 0.2493351   time: 143.22\n",
      "===> Epoch 157 Complete: Avg. Loss: 0.2494703   time: 143.07\n",
      "===> Epoch 158 Complete: Avg. Loss: 0.2500003   time: 143.20\n",
      "===> Epoch 159 Complete: Avg. Loss: 0.2586146   time: 142.90\n",
      "===> Epoch 160 Complete: Avg. Loss: 0.2344070   time: 143.32\n",
      "===> Epoch 161 Complete: Avg. Loss: 0.2407631   time: 142.88\n",
      "===> Epoch 162 Complete: Avg. Loss: 0.2516291   time: 143.01\n",
      "===> Epoch 163 Complete: Avg. Loss: 0.2503225   time: 143.01\n",
      "===> Epoch 164 Complete: Avg. Loss: 0.2523795   time: 143.25\n",
      "===> Epoch 165 Complete: Avg. Loss: 0.2468535   time: 143.70\n",
      "===> Epoch 166 Complete: Avg. Loss: 0.2593192   time: 142.91\n",
      "===> Epoch 167 Complete: Avg. Loss: 0.2444424   time: 143.12\n",
      "===> Epoch 168 Complete: Avg. Loss: 0.2527281   time: 142.70\n",
      "===> Epoch 169 Complete: Avg. Loss: 0.2385026   time: 143.96\n",
      "===> Epoch 170 Complete: Avg. Loss: 0.2473992   time: 143.42\n",
      "===> Epoch 171 Complete: Avg. Loss: 0.2590297   time: 142.69\n",
      "===> Epoch 172 Complete: Avg. Loss: 0.2521018   time: 142.91\n",
      "===> Epoch 173 Complete: Avg. Loss: 0.2381212   time: 143.25\n",
      "===> Epoch 174 Complete: Avg. Loss: 0.2476482   time: 142.81\n",
      "===> Epoch 175 Complete: Avg. Loss: 0.2382851   time: 143.05\n",
      "===> Epoch 176 Complete: Avg. Loss: 0.2529150   time: 142.86\n",
      "===> Epoch 177 Complete: Avg. Loss: 0.2423988   time: 142.46\n",
      "===> Epoch 178 Complete: Avg. Loss: 0.2331389   time: 142.55\n",
      "===> Epoch 179 Complete: Avg. Loss: 0.3174016   time: 142.93\n",
      "===> Epoch 180 Complete: Avg. Loss: 0.2370549   time: 142.95\n",
      "===> Epoch 181 Complete: Avg. Loss: 0.2378491   time: 142.43\n",
      "===> Epoch 182 Complete: Avg. Loss: 0.2545130   time: 142.70\n",
      "===> Epoch 183 Complete: Avg. Loss: 0.2507522   time: 142.60\n",
      "===> Epoch 184 Complete: Avg. Loss: 0.2373465   time: 143.30\n",
      "===> Epoch 185 Complete: Avg. Loss: 0.2299424   time: 142.91\n",
      "===> Epoch 186 Complete: Avg. Loss: 0.2476553   time: 143.30\n",
      "===> Epoch 187 Complete: Avg. Loss: 0.2374996   time: 142.96\n",
      "===> Epoch 188 Complete: Avg. Loss: 0.2550755   time: 142.92\n",
      "===> Epoch 189 Complete: Avg. Loss: 0.2347893   time: 143.02\n",
      "===> Epoch 190 Complete: Avg. Loss: 0.2370712   time: 142.94\n",
      "===> Epoch 191 Complete: Avg. Loss: 0.2305074   time: 143.31\n",
      "===> Epoch 192 Complete: Avg. Loss: 0.3066638   time: 142.45\n",
      "===> Epoch 193 Complete: Avg. Loss: 0.2403465   time: 143.19\n",
      "===> Epoch 194 Complete: Avg. Loss: 0.2444534   time: 142.94\n",
      "===> Epoch 195 Complete: Avg. Loss: 0.2263958   time: 142.93\n",
      "===> Epoch 196 Complete: Avg. Loss: 0.2304094   time: 142.85\n",
      "===> Epoch 197 Complete: Avg. Loss: 0.2247933   time: 142.72\n",
      "===> Epoch 198 Complete: Avg. Loss: 0.2165325   time: 142.81\n",
      "===> Epoch 199 Complete: Avg. Loss: 0.2390379   time: 143.07\n",
      "===> Epoch 200 Complete: Avg. Loss: 0.2237277   time: 143.29\n",
      "[tensor([19.1600, 19.4980, 10.8066, 17.1689, 17.4198, 14.3987, 10.3160, 13.0090,\n",
      "        13.6839, 18.7218, 18.2228, 16.7289, 19.3871, 18.6625, 18.2957]), tensor([25.3727, 25.9443, 13.7800, 23.3723, 23.2483, 18.2583, 14.3142, 16.4494,\n",
      "        17.3235, 25.3983, 23.2074, 21.9740, 25.6248, 23.5236, 22.6558]), tensor([28.0824, 29.4711,  8.6736, 25.4479, 25.6468, 18.6525, 10.3022, 11.0555,\n",
      "        12.8996, 28.2745, 27.0090, 24.1107, 29.1740, 27.7829, 25.5750])]\n",
      "Test result: 22.1439\n",
      "Checkpoint saved to ./model/25_02_09_16_45_Recons_depth_4x(pretrain)/S9_model_epoch_200.pth\n",
      "===> Epoch 201 Complete: Avg. Loss: 0.2440777   time: 142.84\n",
      "===> Epoch 202 Complete: Avg. Loss: 0.2158607   time: 143.01\n",
      "===> Epoch 203 Complete: Avg. Loss: 0.2290508   time: 143.28\n",
      "===> Epoch 204 Complete: Avg. Loss: 0.2240092   time: 143.19\n",
      "===> Epoch 205 Complete: Avg. Loss: 0.2190953   time: 142.90\n",
      "===> Epoch 206 Complete: Avg. Loss: 0.2118617   time: 143.31\n",
      "===> Epoch 207 Complete: Avg. Loss: 0.2149812   time: 142.91\n",
      "===> Epoch 208 Complete: Avg. Loss: 0.2158614   time: 142.70\n",
      "===> Epoch 209 Complete: Avg. Loss: 0.2149892   time: 142.88\n",
      "===> Epoch 210 Complete: Avg. Loss: 0.2287668   time: 142.90\n",
      "===> Epoch 211 Complete: Avg. Loss: 0.2250867   time: 142.92\n",
      "===> Epoch 212 Complete: Avg. Loss: 0.2034080   time: 142.88\n",
      "===> Epoch 213 Complete: Avg. Loss: 0.2147520   time: 142.62\n",
      "===> Epoch 214 Complete: Avg. Loss: 0.2087351   time: 142.72\n",
      "===> Epoch 215 Complete: Avg. Loss: 0.2119796   time: 143.22\n",
      "===> Epoch 216 Complete: Avg. Loss: 0.2279541   time: 143.23\n",
      "===> Epoch 217 Complete: Avg. Loss: 0.2153137   time: 142.83\n",
      "===> Epoch 218 Complete: Avg. Loss: 0.2192466   time: 142.83\n",
      "===> Epoch 219 Complete: Avg. Loss: 0.2017359   time: 143.38\n",
      "===> Epoch 220 Complete: Avg. Loss: 0.2042036   time: 143.24\n",
      "===> Epoch 221 Complete: Avg. Loss: 0.2115483   time: 142.79\n",
      "===> Epoch 222 Complete: Avg. Loss: 0.2232848   time: 142.82\n",
      "===> Epoch 223 Complete: Avg. Loss: 0.2057860   time: 143.20\n",
      "===> Epoch 224 Complete: Avg. Loss: 0.2096490   time: 142.88\n",
      "===> Epoch 225 Complete: Avg. Loss: 0.2217564   time: 143.07\n",
      "===> Epoch 226 Complete: Avg. Loss: 0.2112412   time: 143.03\n",
      "===> Epoch 227 Complete: Avg. Loss: 0.2077790   time: 143.36\n",
      "===> Epoch 228 Complete: Avg. Loss: 0.1969979   time: 142.74\n",
      "===> Epoch 229 Complete: Avg. Loss: 0.2065455   time: 143.00\n",
      "===> Epoch 230 Complete: Avg. Loss: 0.2182111   time: 143.06\n",
      "===> Epoch 231 Complete: Avg. Loss: 0.2051842   time: 143.19\n",
      "===> Epoch 232 Complete: Avg. Loss: 0.2234014   time: 142.73\n",
      "===> Epoch 233 Complete: Avg. Loss: 0.2040504   time: 142.72\n",
      "===> Epoch 234 Complete: Avg. Loss: 0.2124706   time: 143.23\n",
      "===> Epoch 235 Complete: Avg. Loss: 0.2048156   time: 143.04\n",
      "===> Epoch 236 Complete: Avg. Loss: 0.2145764   time: 142.84\n",
      "===> Epoch 237 Complete: Avg. Loss: 0.2163491   time: 142.63\n",
      "===> Epoch 238 Complete: Avg. Loss: 0.2190014   time: 143.05\n",
      "===> Epoch 239 Complete: Avg. Loss: 0.2164959   time: 143.05\n",
      "===> Epoch 240 Complete: Avg. Loss: 0.2040331   time: 142.99\n",
      "===> Epoch 241 Complete: Avg. Loss: 0.2159483   time: 142.83\n",
      "===> Epoch 242 Complete: Avg. Loss: 0.2191239   time: 142.90\n",
      "===> Epoch 243 Complete: Avg. Loss: 0.2004679   time: 142.64\n",
      "===> Epoch 244 Complete: Avg. Loss: 0.2329171   time: 142.97\n",
      "===> Epoch 245 Complete: Avg. Loss: 0.1938675   time: 142.91\n",
      "===> Epoch 246 Complete: Avg. Loss: 0.2111056   time: 143.10\n",
      "===> Epoch 247 Complete: Avg. Loss: 0.2115940   time: 142.60\n",
      "===> Epoch 248 Complete: Avg. Loss: 0.2025885   time: 143.16\n",
      "===> Epoch 249 Complete: Avg. Loss: 0.2214728   time: 142.78\n",
      "===> Epoch 250 Complete: Avg. Loss: 0.1987065   time: 142.90\n",
      "===> Epoch 251 Complete: Avg. Loss: 0.1926149   time: 142.76\n",
      "===> Epoch 252 Complete: Avg. Loss: 0.2530401   time: 142.91\n",
      "===> Epoch 253 Complete: Avg. Loss: 0.1896760   time: 142.91\n",
      "===> Epoch 254 Complete: Avg. Loss: 0.1906823   time: 142.65\n",
      "===> Epoch 255 Complete: Avg. Loss: 0.1961920   time: 143.01\n",
      "===> Epoch 256 Complete: Avg. Loss: 0.1966561   time: 142.82\n",
      "===> Epoch 257 Complete: Avg. Loss: 0.1962444   time: 142.97\n",
      "===> Epoch 258 Complete: Avg. Loss: 0.1935981   time: 143.38\n",
      "===> Epoch 259 Complete: Avg. Loss: 0.1957191   time: 143.12\n",
      "===> Epoch 260 Complete: Avg. Loss: 0.1959096   time: 143.08\n",
      "===> Epoch 261 Complete: Avg. Loss: 0.1980642   time: 143.05\n",
      "===> Epoch 262 Complete: Avg. Loss: 0.2047449   time: 142.32\n",
      "===> Epoch 263 Complete: Avg. Loss: 0.1992335   time: 142.67\n",
      "===> Epoch 264 Complete: Avg. Loss: 0.1908128   time: 143.15\n",
      "===> Epoch 265 Complete: Avg. Loss: 0.1931787   time: 143.06\n",
      "===> Epoch 266 Complete: Avg. Loss: 0.1870086   time: 142.93\n",
      "===> Epoch 267 Complete: Avg. Loss: 0.1874573   time: 143.10\n",
      "===> Epoch 268 Complete: Avg. Loss: 0.1834747   time: 143.58\n",
      "===> Epoch 269 Complete: Avg. Loss: 0.1890500   time: 142.92\n",
      "===> Epoch 270 Complete: Avg. Loss: 0.1990640   time: 142.66\n",
      "===> Epoch 271 Complete: Avg. Loss: 0.1952811   time: 142.56\n",
      "===> Epoch 272 Complete: Avg. Loss: 0.2003334   time: 142.80\n",
      "===> Epoch 273 Complete: Avg. Loss: 0.1966849   time: 142.81\n",
      "===> Epoch 274 Complete: Avg. Loss: 0.1906959   time: 142.90\n",
      "===> Epoch 275 Complete: Avg. Loss: 0.2437647   time: 142.75\n",
      "===> Epoch 276 Complete: Avg. Loss: 0.1958371   time: 142.84\n",
      "===> Epoch 277 Complete: Avg. Loss: 0.1976277   time: 142.84\n",
      "===> Epoch 278 Complete: Avg. Loss: 0.1966203   time: 142.89\n",
      "===> Epoch 279 Complete: Avg. Loss: 0.1905161   time: 142.57\n",
      "===> Epoch 280 Complete: Avg. Loss: 0.1890297   time: 143.17\n",
      "===> Epoch 281 Complete: Avg. Loss: 0.1903426   time: 142.53\n",
      "===> Epoch 282 Complete: Avg. Loss: 0.1910700   time: 142.78\n",
      "===> Epoch 283 Complete: Avg. Loss: 0.2113325   time: 143.46\n",
      "===> Epoch 284 Complete: Avg. Loss: 0.1904678   time: 143.14\n",
      "===> Epoch 285 Complete: Avg. Loss: 0.1908040   time: 143.21\n",
      "===> Epoch 286 Complete: Avg. Loss: 0.1968801   time: 142.85\n",
      "===> Epoch 287 Complete: Avg. Loss: 0.1983200   time: 143.01\n",
      "===> Epoch 288 Complete: Avg. Loss: 0.1964580   time: 143.02\n",
      "===> Epoch 289 Complete: Avg. Loss: 0.1942948   time: 142.96\n",
      "===> Epoch 290 Complete: Avg. Loss: 0.2017303   time: 143.40\n",
      "===> Epoch 291 Complete: Avg. Loss: 0.1873422   time: 142.95\n",
      "===> Epoch 292 Complete: Avg. Loss: 0.2108242   time: 143.05\n",
      "===> Epoch 293 Complete: Avg. Loss: 0.1872360   time: 143.05\n",
      "===> Epoch 294 Complete: Avg. Loss: 0.1945708   time: 143.27\n",
      "===> Epoch 295 Complete: Avg. Loss: 0.1948410   time: 142.80\n",
      "===> Epoch 296 Complete: Avg. Loss: 0.1954190   time: 143.21\n",
      "===> Epoch 297 Complete: Avg. Loss: 0.1864449   time: 142.86\n",
      "===> Epoch 298 Complete: Avg. Loss: 0.1813174   time: 143.23\n",
      "===> Epoch 299 Complete: Avg. Loss: 0.1877520   time: 142.94\n",
      "===> Epoch 300 Complete: Avg. Loss: 0.1832495   time: 143.15\n",
      "[tensor([19.1600, 19.4980, 10.8066, 17.1689, 17.4198, 14.3987, 10.3160, 13.0090,\n",
      "        13.6839, 18.7218, 18.2228, 16.7289, 19.3871, 18.6625, 18.2957]), tensor([25.3727, 25.9443, 13.7800, 23.3723, 23.2483, 18.2583, 14.3142, 16.4494,\n",
      "        17.3235, 25.3983, 23.2074, 21.9740, 25.6248, 23.5236, 22.6558]), tensor([28.0824, 29.4711,  8.6736, 25.4479, 25.6468, 18.6525, 10.3022, 11.0555,\n",
      "        12.8996, 28.2745, 27.0090, 24.1107, 29.1740, 27.7829, 25.5750]), tensor([31.0471, 31.8496, 13.3527, 26.4463, 27.2928, 20.1460, 13.9903, 16.5118,\n",
      "        17.5694, 29.9586, 28.5552, 25.2103, 30.5194, 29.2836, 26.6949])]\n",
      "Test result: 24.5619\n",
      "Checkpoint saved to ./model/25_02_09_16_45_Recons_depth_4x(pretrain)/S9_model_epoch_300.pth\n",
      "===> Epoch 301 Complete: Avg. Loss: 0.1847401   time: 142.57\n",
      "===> Epoch 302 Complete: Avg. Loss: 0.1718842   time: 143.38\n",
      "===> Epoch 303 Complete: Avg. Loss: 0.1815143   time: 143.42\n",
      "===> Epoch 304 Complete: Avg. Loss: 0.1801757   time: 143.32\n",
      "===> Epoch 305 Complete: Avg. Loss: 0.1789666   time: 142.91\n",
      "===> Epoch 306 Complete: Avg. Loss: 0.1779838   time: 143.35\n",
      "===> Epoch 307 Complete: Avg. Loss: 0.1835985   time: 142.89\n",
      "===> Epoch 308 Complete: Avg. Loss: 0.1705820   time: 143.17\n",
      "===> Epoch 309 Complete: Avg. Loss: 0.1760970   time: 142.73\n",
      "===> Epoch 310 Complete: Avg. Loss: 0.1866661   time: 143.10\n",
      "===> Epoch 311 Complete: Avg. Loss: 0.1870980   time: 142.98\n",
      "===> Epoch 312 Complete: Avg. Loss: 0.1882492   time: 142.42\n",
      "===> Epoch 313 Complete: Avg. Loss: 0.1768753   time: 142.68\n",
      "===> Epoch 314 Complete: Avg. Loss: 0.1838315   time: 142.50\n",
      "===> Epoch 315 Complete: Avg. Loss: 0.1756855   time: 142.92\n",
      "===> Epoch 316 Complete: Avg. Loss: 0.1815823   time: 142.98\n",
      "===> Epoch 317 Complete: Avg. Loss: 0.1767581   time: 142.57\n",
      "===> Epoch 318 Complete: Avg. Loss: 0.1782018   time: 142.77\n",
      "===> Epoch 319 Complete: Avg. Loss: 0.1858348   time: 142.25\n",
      "===> Epoch 320 Complete: Avg. Loss: 0.1866004   time: 142.56\n",
      "===> Epoch 321 Complete: Avg. Loss: 0.1765922   time: 142.72\n",
      "===> Epoch 322 Complete: Avg. Loss: 0.1778824   time: 142.90\n",
      "===> Epoch 323 Complete: Avg. Loss: 0.1759729   time: 143.45\n",
      "===> Epoch 324 Complete: Avg. Loss: 0.1843987   time: 142.71\n",
      "===> Epoch 325 Complete: Avg. Loss: 0.1732651   time: 142.88\n",
      "===> Epoch 326 Complete: Avg. Loss: 0.1767124   time: 142.72\n",
      "===> Epoch 327 Complete: Avg. Loss: 0.1799054   time: 142.92\n",
      "===> Epoch 328 Complete: Avg. Loss: 0.1884865   time: 143.16\n",
      "===> Epoch 329 Complete: Avg. Loss: 0.1768352   time: 142.60\n",
      "===> Epoch 330 Complete: Avg. Loss: 0.1726756   time: 143.18\n",
      "===> Epoch 331 Complete: Avg. Loss: 0.1723690   time: 142.61\n",
      "===> Epoch 332 Complete: Avg. Loss: 0.1654752   time: 142.84\n",
      "===> Epoch 333 Complete: Avg. Loss: 0.1747533   time: 142.73\n",
      "===> Epoch 334 Complete: Avg. Loss: 0.1720819   time: 143.04\n",
      "===> Epoch 335 Complete: Avg. Loss: 0.1733656   time: 143.05\n",
      "===> Epoch 336 Complete: Avg. Loss: 0.1771666   time: 143.27\n",
      "===> Epoch 337 Complete: Avg. Loss: 0.1851909   time: 143.41\n",
      "===> Epoch 338 Complete: Avg. Loss: 0.1709136   time: 143.08\n",
      "===> Epoch 339 Complete: Avg. Loss: 0.1866338   time: 143.04\n",
      "===> Epoch 340 Complete: Avg. Loss: 0.1895880   time: 143.03\n",
      "===> Epoch 341 Complete: Avg. Loss: 0.1758115   time: 143.64\n",
      "===> Epoch 342 Complete: Avg. Loss: 0.1721741   time: 142.73\n",
      "===> Epoch 343 Complete: Avg. Loss: 0.1731158   time: 143.07\n",
      "===> Epoch 344 Complete: Avg. Loss: 0.1736999   time: 143.01\n",
      "===> Epoch 345 Complete: Avg. Loss: 0.1839012   time: 143.33\n",
      "===> Epoch 346 Complete: Avg. Loss: 0.1743173   time: 143.22\n",
      "===> Epoch 347 Complete: Avg. Loss: 0.1715544   time: 142.72\n",
      "===> Epoch 348 Complete: Avg. Loss: 0.1690122   time: 142.99\n",
      "===> Epoch 349 Complete: Avg. Loss: 0.1808641   time: 142.72\n",
      "===> Epoch 350 Complete: Avg. Loss: 0.1820791   time: 143.25\n",
      "===> Epoch 351 Complete: Avg. Loss: 0.1691095   time: 142.89\n",
      "===> Epoch 352 Complete: Avg. Loss: 0.1665240   time: 142.60\n",
      "===> Epoch 353 Complete: Avg. Loss: 0.1766238   time: 142.83\n",
      "===> Epoch 354 Complete: Avg. Loss: 0.1674801   time: 142.93\n",
      "===> Epoch 355 Complete: Avg. Loss: 0.1670608   time: 142.83\n",
      "===> Epoch 356 Complete: Avg. Loss: 0.1672598   time: 142.98\n",
      "===> Epoch 357 Complete: Avg. Loss: 0.1765353   time: 142.46\n",
      "===> Epoch 358 Complete: Avg. Loss: 0.1689543   time: 142.96\n",
      "===> Epoch 359 Complete: Avg. Loss: 0.1822237   time: 143.22\n",
      "===> Epoch 360 Complete: Avg. Loss: 0.1683283   time: 143.27\n",
      "===> Epoch 361 Complete: Avg. Loss: 0.1601434   time: 142.91\n",
      "===> Epoch 362 Complete: Avg. Loss: 0.1729636   time: 142.94\n",
      "===> Epoch 363 Complete: Avg. Loss: 0.1737944   time: 142.94\n",
      "===> Epoch 364 Complete: Avg. Loss: 0.1695984   time: 143.15\n",
      "===> Epoch 365 Complete: Avg. Loss: 0.1731341   time: 143.03\n",
      "===> Epoch 366 Complete: Avg. Loss: 0.1878281   time: 143.04\n",
      "===> Epoch 367 Complete: Avg. Loss: 0.1735279   time: 143.02\n",
      "===> Epoch 368 Complete: Avg. Loss: 0.1684710   time: 142.81\n",
      "===> Epoch 369 Complete: Avg. Loss: 0.1739346   time: 142.97\n",
      "===> Epoch 370 Complete: Avg. Loss: 0.1672573   time: 142.91\n",
      "===> Epoch 371 Complete: Avg. Loss: 0.1693232   time: 143.06\n",
      "===> Epoch 372 Complete: Avg. Loss: 0.1718846   time: 143.57\n",
      "===> Epoch 373 Complete: Avg. Loss: 0.1620515   time: 143.11\n",
      "===> Epoch 374 Complete: Avg. Loss: 0.1761498   time: 143.08\n",
      "===> Epoch 375 Complete: Avg. Loss: 0.1664034   time: 143.16\n",
      "===> Epoch 376 Complete: Avg. Loss: 0.1659762   time: 143.07\n",
      "===> Epoch 377 Complete: Avg. Loss: 0.1677924   time: 143.08\n",
      "===> Epoch 378 Complete: Avg. Loss: 0.1589083   time: 142.62\n",
      "===> Epoch 379 Complete: Avg. Loss: 0.1741874   time: 142.91\n",
      "===> Epoch 380 Complete: Avg. Loss: 0.1720054   time: 143.02\n",
      "===> Epoch 381 Complete: Avg. Loss: 0.1642320   time: 142.94\n",
      "===> Epoch 382 Complete: Avg. Loss: 0.1713098   time: 143.10\n",
      "===> Epoch 383 Complete: Avg. Loss: 0.1635827   time: 143.02\n",
      "===> Epoch 384 Complete: Avg. Loss: 0.1596530   time: 142.93\n",
      "===> Epoch 385 Complete: Avg. Loss: 0.1628948   time: 142.47\n",
      "===> Epoch 386 Complete: Avg. Loss: 0.1645801   time: 143.18\n",
      "===> Epoch 387 Complete: Avg. Loss: 0.1766903   time: 142.99\n",
      "===> Epoch 388 Complete: Avg. Loss: 0.2375121   time: 142.93\n",
      "===> Epoch 389 Complete: Avg. Loss: 0.1608529   time: 143.18\n",
      "===> Epoch 390 Complete: Avg. Loss: 0.1701045   time: 142.97\n",
      "===> Epoch 391 Complete: Avg. Loss: 0.1686074   time: 143.27\n",
      "===> Epoch 392 Complete: Avg. Loss: 0.1674847   time: 143.19\n",
      "===> Epoch 393 Complete: Avg. Loss: 0.1701384   time: 142.88\n",
      "===> Epoch 394 Complete: Avg. Loss: 0.1646534   time: 142.65\n",
      "===> Epoch 395 Complete: Avg. Loss: 0.1661359   time: 142.58\n",
      "===> Epoch 396 Complete: Avg. Loss: 0.1771596   time: 142.41\n",
      "===> Epoch 397 Complete: Avg. Loss: 0.1830923   time: 142.58\n",
      "===> Epoch 398 Complete: Avg. Loss: 0.1622386   time: 142.85\n",
      "===> Epoch 399 Complete: Avg. Loss: 0.1566481   time: 142.80\n",
      "===> Epoch 400 Complete: Avg. Loss: 0.1653789   time: 142.81\n",
      "[tensor([19.1600, 19.4980, 10.8066, 17.1689, 17.4198, 14.3987, 10.3160, 13.0090,\n",
      "        13.6839, 18.7218, 18.2228, 16.7289, 19.3871, 18.6625, 18.2957]), tensor([25.3727, 25.9443, 13.7800, 23.3723, 23.2483, 18.2583, 14.3142, 16.4494,\n",
      "        17.3235, 25.3983, 23.2074, 21.9740, 25.6248, 23.5236, 22.6558]), tensor([28.0824, 29.4711,  8.6736, 25.4479, 25.6468, 18.6525, 10.3022, 11.0555,\n",
      "        12.8996, 28.2745, 27.0090, 24.1107, 29.1740, 27.7829, 25.5750]), tensor([31.0471, 31.8496, 13.3527, 26.4463, 27.2928, 20.1460, 13.9903, 16.5118,\n",
      "        17.5694, 29.9586, 28.5552, 25.2103, 30.5194, 29.2836, 26.6949]), tensor([31.2505, 31.9779, 10.5622, 27.6869, 27.8998, 21.5530, 12.2783, 13.7427,\n",
      "        13.2072, 31.2582, 30.4114, 26.8475, 32.2056, 31.1638, 28.5424])]\n",
      "Test result: 24.7058\n",
      "Checkpoint saved to ./model/25_02_09_16_45_Recons_depth_4x(pretrain)/S9_model_epoch_400.pth\n",
      "===> Epoch 401 Complete: Avg. Loss: 0.1699064   time: 142.52\n",
      "===> Epoch 402 Complete: Avg. Loss: 0.1540569   time: 143.28\n",
      "===> Epoch 403 Complete: Avg. Loss: 0.1598905   time: 142.79\n",
      "===> Epoch 404 Complete: Avg. Loss: 0.1718666   time: 143.05\n",
      "===> Epoch 405 Complete: Avg. Loss: 0.1626945   time: 142.84\n",
      "===> Epoch 406 Complete: Avg. Loss: 0.1646801   time: 142.75\n",
      "===> Epoch 407 Complete: Avg. Loss: 0.1552853   time: 142.88\n",
      "===> Epoch 408 Complete: Avg. Loss: 0.1602158   time: 143.10\n",
      "===> Epoch 409 Complete: Avg. Loss: 0.1607403   time: 143.16\n",
      "===> Epoch 410 Complete: Avg. Loss: 0.1747469   time: 143.03\n",
      "===> Epoch 411 Complete: Avg. Loss: 0.1575286   time: 143.33\n",
      "===> Epoch 412 Complete: Avg. Loss: 0.1731011   time: 142.90\n",
      "===> Epoch 413 Complete: Avg. Loss: 0.1576058   time: 143.05\n",
      "===> Epoch 414 Complete: Avg. Loss: 0.1489917   time: 143.03\n",
      "===> Epoch 415 Complete: Avg. Loss: 0.1619542   time: 142.67\n",
      "===> Epoch 416 Complete: Avg. Loss: 0.1556775   time: 143.07\n",
      "===> Epoch 417 Complete: Avg. Loss: 0.1552481   time: 142.69\n",
      "===> Epoch 418 Complete: Avg. Loss: 0.1570704   time: 142.83\n",
      "===> Epoch 419 Complete: Avg. Loss: 0.1493170   time: 142.95\n",
      "===> Epoch 420 Complete: Avg. Loss: 0.1625302   time: 143.27\n",
      "===> Epoch 421 Complete: Avg. Loss: 0.1589168   time: 142.98\n",
      "===> Epoch 422 Complete: Avg. Loss: 0.1586102   time: 143.11\n",
      "===> Epoch 423 Complete: Avg. Loss: 0.1594359   time: 142.82\n",
      "===> Epoch 424 Complete: Avg. Loss: 0.1492293   time: 142.67\n",
      "===> Epoch 425 Complete: Avg. Loss: 0.1530110   time: 142.96\n",
      "===> Epoch 426 Complete: Avg. Loss: 0.1565710   time: 142.97\n",
      "===> Epoch 427 Complete: Avg. Loss: 0.1505006   time: 142.98\n",
      "===> Epoch 428 Complete: Avg. Loss: 0.1602492   time: 142.92\n",
      "===> Epoch 429 Complete: Avg. Loss: 0.1619768   time: 142.81\n",
      "===> Epoch 430 Complete: Avg. Loss: 0.1588508   time: 143.28\n",
      "===> Epoch 431 Complete: Avg. Loss: 0.1613709   time: 142.52\n",
      "===> Epoch 432 Complete: Avg. Loss: 0.1513320   time: 142.79\n",
      "===> Epoch 433 Complete: Avg. Loss: 0.1514072   time: 142.83\n",
      "===> Epoch 434 Complete: Avg. Loss: 0.1534023   time: 143.24\n",
      "===> Epoch 435 Complete: Avg. Loss: 0.1559981   time: 143.00\n",
      "===> Epoch 436 Complete: Avg. Loss: 0.1527979   time: 142.86\n",
      "===> Epoch 437 Complete: Avg. Loss: 0.1588568   time: 143.09\n",
      "===> Epoch 438 Complete: Avg. Loss: 0.1547841   time: 142.63\n",
      "===> Epoch 439 Complete: Avg. Loss: 0.1558329   time: 142.62\n",
      "===> Epoch 440 Complete: Avg. Loss: 0.1537127   time: 143.18\n",
      "===> Epoch 441 Complete: Avg. Loss: 0.1531998   time: 143.10\n",
      "===> Epoch 442 Complete: Avg. Loss: 0.1600625   time: 142.60\n",
      "===> Epoch 443 Complete: Avg. Loss: 0.1601708   time: 143.34\n",
      "===> Epoch 444 Complete: Avg. Loss: 0.1575165   time: 142.91\n",
      "===> Epoch 445 Complete: Avg. Loss: 0.1712022   time: 142.80\n",
      "===> Epoch 446 Complete: Avg. Loss: 0.1581521   time: 143.20\n",
      "===> Epoch 447 Complete: Avg. Loss: 0.1462939   time: 143.01\n",
      "===> Epoch 448 Complete: Avg. Loss: 0.1547093   time: 142.77\n",
      "===> Epoch 449 Complete: Avg. Loss: 0.1604510   time: 142.84\n",
      "===> Epoch 450 Complete: Avg. Loss: 0.1576235   time: 142.92\n",
      "===> Epoch 451 Complete: Avg. Loss: 0.1622844   time: 143.06\n",
      "===> Epoch 452 Complete: Avg. Loss: 0.1572462   time: 142.96\n",
      "===> Epoch 453 Complete: Avg. Loss: 0.1494533   time: 143.52\n",
      "===> Epoch 454 Complete: Avg. Loss: 0.1540823   time: 142.86\n",
      "===> Epoch 455 Complete: Avg. Loss: 0.1480518   time: 142.92\n",
      "===> Epoch 456 Complete: Avg. Loss: 0.1468332   time: 142.94\n",
      "===> Epoch 457 Complete: Avg. Loss: 0.1444075   time: 142.48\n",
      "===> Epoch 458 Complete: Avg. Loss: 0.1576399   time: 142.65\n",
      "===> Epoch 459 Complete: Avg. Loss: 0.1658837   time: 142.84\n",
      "===> Epoch 460 Complete: Avg. Loss: 0.1470229   time: 142.65\n",
      "===> Epoch 461 Complete: Avg. Loss: 0.1484256   time: 142.74\n",
      "===> Epoch 462 Complete: Avg. Loss: 0.1573876   time: 142.81\n",
      "===> Epoch 463 Complete: Avg. Loss: 0.1464544   time: 143.43\n",
      "===> Epoch 464 Complete: Avg. Loss: 0.1481071   time: 143.01\n",
      "===> Epoch 465 Complete: Avg. Loss: 0.1541279   time: 143.07\n",
      "===> Epoch 466 Complete: Avg. Loss: 0.1432792   time: 142.97\n",
      "===> Epoch 467 Complete: Avg. Loss: 0.1467698   time: 143.73\n",
      "===> Epoch 468 Complete: Avg. Loss: 0.1516517   time: 142.98\n",
      "===> Epoch 469 Complete: Avg. Loss: 0.1476793   time: 142.66\n",
      "===> Epoch 470 Complete: Avg. Loss: 0.1552336   time: 143.02\n",
      "===> Epoch 471 Complete: Avg. Loss: 0.1456322   time: 143.11\n",
      "===> Epoch 472 Complete: Avg. Loss: 0.1510456   time: 143.46\n",
      "===> Epoch 473 Complete: Avg. Loss: 0.1484246   time: 143.56\n",
      "===> Epoch 474 Complete: Avg. Loss: 0.1567614   time: 142.84\n",
      "===> Epoch 475 Complete: Avg. Loss: 0.1520954   time: 143.39\n",
      "===> Epoch 476 Complete: Avg. Loss: 0.1551763   time: 143.28\n",
      "===> Epoch 477 Complete: Avg. Loss: 0.1440896   time: 142.56\n",
      "===> Epoch 478 Complete: Avg. Loss: 0.1440584   time: 142.05\n",
      "===> Epoch 479 Complete: Avg. Loss: 0.1458165   time: 142.05\n",
      "===> Epoch 480 Complete: Avg. Loss: 0.1481100   time: 142.05\n",
      "===> Epoch 481 Complete: Avg. Loss: 0.1547735   time: 142.03\n",
      "===> Epoch 482 Complete: Avg. Loss: 0.1515364   time: 141.94\n",
      "===> Epoch 483 Complete: Avg. Loss: 0.1425027   time: 141.98\n",
      "===> Epoch 484 Complete: Avg. Loss: 0.1480472   time: 142.15\n",
      "===> Epoch 485 Complete: Avg. Loss: 0.1505387   time: 142.26\n",
      "===> Epoch 486 Complete: Avg. Loss: 0.1484196   time: 142.60\n",
      "===> Epoch 487 Complete: Avg. Loss: 0.1407014   time: 142.23\n",
      "===> Epoch 488 Complete: Avg. Loss: 0.1394717   time: 141.89\n",
      "===> Epoch 489 Complete: Avg. Loss: 0.1429436   time: 142.07\n",
      "===> Epoch 490 Complete: Avg. Loss: 0.1471738   time: 142.11\n",
      "===> Epoch 491 Complete: Avg. Loss: 0.1444165   time: 142.25\n",
      "===> Epoch 492 Complete: Avg. Loss: 0.1538877   time: 142.46\n",
      "===> Epoch 493 Complete: Avg. Loss: 0.1439379   time: 142.17\n",
      "===> Epoch 494 Complete: Avg. Loss: 0.1533039   time: 142.14\n",
      "===> Epoch 495 Complete: Avg. Loss: 0.1601039   time: 141.66\n",
      "===> Epoch 496 Complete: Avg. Loss: 0.1492334   time: 142.19\n",
      "===> Epoch 497 Complete: Avg. Loss: 0.1508292   time: 142.06\n",
      "===> Epoch 498 Complete: Avg. Loss: 0.1412675   time: 142.21\n",
      "===> Epoch 499 Complete: Avg. Loss: 0.1483167   time: 142.07\n",
      "===> Epoch 500 Complete: Avg. Loss: 0.1520293   time: 142.44\n",
      "[tensor([19.1600, 19.4980, 10.8066, 17.1689, 17.4198, 14.3987, 10.3160, 13.0090,\n",
      "        13.6839, 18.7218, 18.2228, 16.7289, 19.3871, 18.6625, 18.2957]), tensor([25.3727, 25.9443, 13.7800, 23.3723, 23.2483, 18.2583, 14.3142, 16.4494,\n",
      "        17.3235, 25.3983, 23.2074, 21.9740, 25.6248, 23.5236, 22.6558]), tensor([28.0824, 29.4711,  8.6736, 25.4479, 25.6468, 18.6525, 10.3022, 11.0555,\n",
      "        12.8996, 28.2745, 27.0090, 24.1107, 29.1740, 27.7829, 25.5750]), tensor([31.0471, 31.8496, 13.3527, 26.4463, 27.2928, 20.1460, 13.9903, 16.5118,\n",
      "        17.5694, 29.9586, 28.5552, 25.2103, 30.5194, 29.2836, 26.6949]), tensor([31.2505, 31.9779, 10.5622, 27.6869, 27.8998, 21.5530, 12.2783, 13.7427,\n",
      "        13.2072, 31.2582, 30.4114, 26.8475, 32.2056, 31.1638, 28.5424]), tensor([32.8996, 33.8295, 10.5872, 28.9906, 28.7111, 23.1501, 12.0393, 13.1413,\n",
      "        13.4252, 32.7423, 31.8044, 27.5045, 33.4359, 32.6657, 29.0726])]\n",
      "Test result: 25.6000\n",
      "Checkpoint saved to ./model/25_02_09_16_45_Recons_depth_4x(pretrain)/S9_model_epoch_500.pth\n",
      "===> Epoch 501 Complete: Avg. Loss: 0.1397276   time: 142.12\n",
      "===> Epoch 502 Complete: Avg. Loss: 0.1519245   time: 142.10\n",
      "===> Epoch 503 Complete: Avg. Loss: 0.1542961   time: 141.98\n",
      "===> Epoch 504 Complete: Avg. Loss: 0.1437239   time: 142.02\n",
      "===> Epoch 505 Complete: Avg. Loss: 0.1484418   time: 142.30\n",
      "===> Epoch 506 Complete: Avg. Loss: 0.1478908   time: 142.52\n",
      "===> Epoch 507 Complete: Avg. Loss: 0.1488780   time: 141.97\n",
      "===> Epoch 508 Complete: Avg. Loss: 0.1410408   time: 142.04\n",
      "===> Epoch 509 Complete: Avg. Loss: 0.1428890   time: 142.11\n",
      "===> Epoch 510 Complete: Avg. Loss: 0.1483286   time: 142.36\n",
      "===> Epoch 511 Complete: Avg. Loss: 0.1472518   time: 142.07\n",
      "===> Epoch 512 Complete: Avg. Loss: 0.1466217   time: 141.78\n",
      "===> Epoch 513 Complete: Avg. Loss: 0.1418649   time: 141.95\n",
      "===> Epoch 514 Complete: Avg. Loss: 0.1393931   time: 142.31\n",
      "===> Epoch 515 Complete: Avg. Loss: 0.1441635   time: 142.37\n",
      "===> Epoch 516 Complete: Avg. Loss: 0.1392317   time: 142.10\n",
      "===> Epoch 517 Complete: Avg. Loss: 0.1410878   time: 142.19\n",
      "===> Epoch 518 Complete: Avg. Loss: 0.1421857   time: 142.40\n",
      "===> Epoch 519 Complete: Avg. Loss: 0.1490746   time: 142.13\n",
      "===> Epoch 520 Complete: Avg. Loss: 0.1424991   time: 142.24\n",
      "===> Epoch 521 Complete: Avg. Loss: 0.1405828   time: 142.29\n",
      "===> Epoch 522 Complete: Avg. Loss: 0.1419481   time: 142.14\n",
      "===> Epoch 523 Complete: Avg. Loss: 0.1475732   time: 142.00\n",
      "===> Epoch 524 Complete: Avg. Loss: 0.1419787   time: 142.09\n",
      "===> Epoch 525 Complete: Avg. Loss: 0.1542194   time: 141.99\n",
      "===> Epoch 526 Complete: Avg. Loss: 0.1528663   time: 141.94\n",
      "===> Epoch 527 Complete: Avg. Loss: 0.1582144   time: 141.95\n",
      "===> Epoch 528 Complete: Avg. Loss: 0.1471411   time: 141.92\n",
      "===> Epoch 529 Complete: Avg. Loss: 0.2017568   time: 142.01\n",
      "===> Epoch 530 Complete: Avg. Loss: 0.1489865   time: 141.80\n",
      "===> Epoch 531 Complete: Avg. Loss: 0.1514885   time: 141.96\n",
      "===> Epoch 532 Complete: Avg. Loss: 0.1424581   time: 142.07\n",
      "===> Epoch 533 Complete: Avg. Loss: 0.1426957   time: 142.09\n",
      "===> Epoch 534 Complete: Avg. Loss: 0.1474445   time: 142.43\n",
      "===> Epoch 535 Complete: Avg. Loss: 0.1404534   time: 141.71\n",
      "===> Epoch 536 Complete: Avg. Loss: 0.1398662   time: 142.17\n",
      "===> Epoch 537 Complete: Avg. Loss: 0.1452769   time: 142.19\n",
      "===> Epoch 538 Complete: Avg. Loss: 0.1454339   time: 141.77\n",
      "===> Epoch 539 Complete: Avg. Loss: 0.1505148   time: 142.05\n",
      "===> Epoch 540 Complete: Avg. Loss: 0.1429903   time: 141.73\n",
      "===> Epoch 541 Complete: Avg. Loss: 0.1635953   time: 141.87\n",
      "===> Epoch 542 Complete: Avg. Loss: 0.1444821   time: 142.33\n",
      "===> Epoch 543 Complete: Avg. Loss: 0.1420688   time: 141.77\n",
      "===> Epoch 544 Complete: Avg. Loss: 0.1436374   time: 142.12\n",
      "===> Epoch 545 Complete: Avg. Loss: 0.1588333   time: 142.09\n",
      "===> Epoch 546 Complete: Avg. Loss: 0.1392140   time: 142.01\n",
      "===> Epoch 547 Complete: Avg. Loss: 0.1414072   time: 142.01\n",
      "===> Epoch 548 Complete: Avg. Loss: 0.1415180   time: 141.82\n",
      "===> Epoch 549 Complete: Avg. Loss: 0.1543084   time: 142.31\n",
      "===> Epoch 550 Complete: Avg. Loss: 0.1404282   time: 142.41\n",
      "===> Epoch 551 Complete: Avg. Loss: 0.1419353   time: 142.05\n",
      "===> Epoch 552 Complete: Avg. Loss: 0.1441434   time: 142.14\n",
      "===> Epoch 553 Complete: Avg. Loss: 0.1395935   time: 142.38\n",
      "===> Epoch 554 Complete: Avg. Loss: 0.1475424   time: 141.58\n",
      "===> Epoch 555 Complete: Avg. Loss: 0.1515042   time: 142.26\n",
      "===> Epoch 556 Complete: Avg. Loss: 0.1425225   time: 142.14\n",
      "===> Epoch 557 Complete: Avg. Loss: 0.1428995   time: 141.92\n",
      "===> Epoch 558 Complete: Avg. Loss: 0.1440014   time: 142.03\n",
      "===> Epoch 559 Complete: Avg. Loss: 0.1402631   time: 141.92\n",
      "===> Epoch 560 Complete: Avg. Loss: 0.1489653   time: 142.07\n",
      "===> Epoch 561 Complete: Avg. Loss: 0.1393968   time: 141.82\n",
      "===> Epoch 562 Complete: Avg. Loss: 0.1354591   time: 142.10\n",
      "===> Epoch 563 Complete: Avg. Loss: 0.1486473   time: 142.18\n",
      "===> Epoch 564 Complete: Avg. Loss: 0.1479393   time: 141.95\n",
      "===> Epoch 565 Complete: Avg. Loss: 0.1985360   time: 142.08\n",
      "===> Epoch 566 Complete: Avg. Loss: 0.1363060   time: 141.93\n",
      "===> Epoch 567 Complete: Avg. Loss: 0.1424486   time: 142.04\n",
      "===> Epoch 568 Complete: Avg. Loss: 0.1541110   time: 142.21\n",
      "===> Epoch 569 Complete: Avg. Loss: 0.1443170   time: 141.83\n",
      "===> Epoch 570 Complete: Avg. Loss: 0.1487809   time: 141.96\n",
      "===> Epoch 571 Complete: Avg. Loss: 0.1404237   time: 141.89\n",
      "===> Epoch 572 Complete: Avg. Loss: 0.1389252   time: 142.11\n",
      "===> Epoch 573 Complete: Avg. Loss: 0.1399331   time: 141.86\n",
      "===> Epoch 574 Complete: Avg. Loss: 0.1384128   time: 142.12\n",
      "===> Epoch 575 Complete: Avg. Loss: 0.1384715   time: 142.05\n",
      "===> Epoch 576 Complete: Avg. Loss: 0.1403362   time: 141.97\n",
      "===> Epoch 577 Complete: Avg. Loss: 0.2015058   time: 142.07\n",
      "===> Epoch 578 Complete: Avg. Loss: 0.1384432   time: 142.31\n",
      "===> Epoch 579 Complete: Avg. Loss: 0.1454381   time: 142.05\n",
      "===> Epoch 580 Complete: Avg. Loss: 0.1467378   time: 142.51\n",
      "===> Epoch 581 Complete: Avg. Loss: 0.1405538   time: 142.04\n",
      "===> Epoch 582 Complete: Avg. Loss: 0.1374323   time: 142.24\n",
      "===> Epoch 583 Complete: Avg. Loss: 0.1406689   time: 141.86\n",
      "===> Epoch 584 Complete: Avg. Loss: 0.1421757   time: 142.23\n",
      "===> Epoch 585 Complete: Avg. Loss: 0.1431440   time: 141.94\n",
      "===> Epoch 586 Complete: Avg. Loss: 0.1446917   time: 141.96\n",
      "===> Epoch 587 Complete: Avg. Loss: 0.1445427   time: 142.18\n",
      "===> Epoch 588 Complete: Avg. Loss: 0.1441771   time: 142.06\n",
      "===> Epoch 589 Complete: Avg. Loss: 0.1358295   time: 142.08\n",
      "===> Epoch 590 Complete: Avg. Loss: 0.2119346   time: 142.02\n",
      "===> Epoch 591 Complete: Avg. Loss: 0.1509129   time: 142.02\n",
      "===> Epoch 592 Complete: Avg. Loss: 0.1551624   time: 141.97\n",
      "===> Epoch 593 Complete: Avg. Loss: 0.1440779   time: 142.17\n",
      "===> Epoch 594 Complete: Avg. Loss: 0.1368018   time: 142.04\n",
      "===> Epoch 595 Complete: Avg. Loss: 0.1526905   time: 142.12\n",
      "===> Epoch 596 Complete: Avg. Loss: 0.1441974   time: 142.17\n",
      "===> Epoch 597 Complete: Avg. Loss: 0.1363189   time: 142.17\n",
      "===> Epoch 598 Complete: Avg. Loss: 0.1467979   time: 141.68\n",
      "===> Epoch 599 Complete: Avg. Loss: 0.1542724   time: 142.17\n",
      "===> Epoch 600 Complete: Avg. Loss: 0.1442400   time: 141.92\n",
      "[tensor([19.1600, 19.4980, 10.8066, 17.1689, 17.4198, 14.3987, 10.3160, 13.0090,\n",
      "        13.6839, 18.7218, 18.2228, 16.7289, 19.3871, 18.6625, 18.2957]), tensor([25.3727, 25.9443, 13.7800, 23.3723, 23.2483, 18.2583, 14.3142, 16.4494,\n",
      "        17.3235, 25.3983, 23.2074, 21.9740, 25.6248, 23.5236, 22.6558]), tensor([28.0824, 29.4711,  8.6736, 25.4479, 25.6468, 18.6525, 10.3022, 11.0555,\n",
      "        12.8996, 28.2745, 27.0090, 24.1107, 29.1740, 27.7829, 25.5750]), tensor([31.0471, 31.8496, 13.3527, 26.4463, 27.2928, 20.1460, 13.9903, 16.5118,\n",
      "        17.5694, 29.9586, 28.5552, 25.2103, 30.5194, 29.2836, 26.6949]), tensor([31.2505, 31.9779, 10.5622, 27.6869, 27.8998, 21.5530, 12.2783, 13.7427,\n",
      "        13.2072, 31.2582, 30.4114, 26.8475, 32.2056, 31.1638, 28.5424]), tensor([32.8996, 33.8295, 10.5872, 28.9906, 28.7111, 23.1501, 12.0393, 13.1413,\n",
      "        13.4252, 32.7423, 31.8044, 27.5045, 33.4359, 32.6657, 29.0726]), tensor([32.9366, 33.1678,  9.8976, 29.0594, 28.9095, 21.7044, 10.3222, 10.7021,\n",
      "        11.1292, 33.2589, 31.8781, 28.1193, 33.8123, 32.5355, 29.6833])]\n",
      "Test result: 25.1411\n",
      "Checkpoint saved to ./model/25_02_09_16_45_Recons_depth_4x(pretrain)/S9_model_epoch_600.pth\n",
      "===> Epoch 601 Complete: Avg. Loss: 0.1460078   time: 141.91\n",
      "===> Epoch 602 Complete: Avg. Loss: 0.1610839   time: 142.16\n",
      "===> Epoch 603 Complete: Avg. Loss: 0.1437599   time: 142.03\n",
      "===> Epoch 604 Complete: Avg. Loss: 0.1450712   time: 142.15\n",
      "===> Epoch 605 Complete: Avg. Loss: 0.1926129   time: 142.39\n",
      "===> Epoch 606 Complete: Avg. Loss: 0.1503754   time: 142.28\n",
      "===> Epoch 607 Complete: Avg. Loss: 0.1431866   time: 142.13\n",
      "===> Epoch 608 Complete: Avg. Loss: 0.1424145   time: 142.20\n",
      "===> Epoch 609 Complete: Avg. Loss: 0.1434322   time: 142.13\n",
      "===> Epoch 610 Complete: Avg. Loss: 0.1484500   time: 141.95\n",
      "===> Epoch 611 Complete: Avg. Loss: 0.1432465   time: 142.47\n",
      "===> Epoch 612 Complete: Avg. Loss: 0.1474874   time: 141.98\n",
      "===> Epoch 613 Complete: Avg. Loss: 0.1389982   time: 142.23\n",
      "===> Epoch 614 Complete: Avg. Loss: 0.1495157   time: 141.81\n",
      "===> Epoch 615 Complete: Avg. Loss: 0.1364138   time: 141.94\n",
      "===> Epoch 616 Complete: Avg. Loss: 0.1428729   time: 142.36\n",
      "===> Epoch 617 Complete: Avg. Loss: 0.1451814   time: 142.26\n",
      "===> Epoch 618 Complete: Avg. Loss: 0.1484806   time: 142.32\n",
      "===> Epoch 619 Complete: Avg. Loss: 0.1431275   time: 142.43\n",
      "===> Epoch 620 Complete: Avg. Loss: 0.1518069   time: 142.11\n",
      "===> Epoch 621 Complete: Avg. Loss: 0.1422396   time: 142.20\n",
      "===> Epoch 622 Complete: Avg. Loss: 0.1415742   time: 142.20\n",
      "===> Epoch 623 Complete: Avg. Loss: 0.1376043   time: 142.00\n",
      "===> Epoch 624 Complete: Avg. Loss: 0.1331476   time: 142.09\n",
      "===> Epoch 625 Complete: Avg. Loss: 0.1403535   time: 142.48\n",
      "===> Epoch 626 Complete: Avg. Loss: 0.1407429   time: 142.43\n",
      "===> Epoch 627 Complete: Avg. Loss: 0.1443468   time: 141.98\n",
      "===> Epoch 628 Complete: Avg. Loss: 0.1415371   time: 142.21\n",
      "===> Epoch 629 Complete: Avg. Loss: 0.1454156   time: 142.30\n",
      "===> Epoch 630 Complete: Avg. Loss: 0.1443888   time: 142.24\n",
      "===> Epoch 631 Complete: Avg. Loss: 0.1948088   time: 142.00\n",
      "===> Epoch 632 Complete: Avg. Loss: 0.1983123   time: 142.27\n",
      "===> Epoch 633 Complete: Avg. Loss: 0.1415191   time: 142.26\n",
      "===> Epoch 634 Complete: Avg. Loss: 0.1392922   time: 142.22\n",
      "===> Epoch 635 Complete: Avg. Loss: 0.1461592   time: 141.97\n",
      "===> Epoch 636 Complete: Avg. Loss: 0.1403541   time: 142.07\n",
      "===> Epoch 637 Complete: Avg. Loss: 0.1363116   time: 141.99\n",
      "===> Epoch 638 Complete: Avg. Loss: 0.1919336   time: 142.10\n",
      "===> Epoch 639 Complete: Avg. Loss: 0.1367223   time: 142.15\n",
      "===> Epoch 640 Complete: Avg. Loss: 0.1383166   time: 142.02\n",
      "===> Epoch 641 Complete: Avg. Loss: 0.1436400   time: 142.15\n",
      "===> Epoch 642 Complete: Avg. Loss: 0.1972908   time: 142.93\n",
      "===> Epoch 643 Complete: Avg. Loss: 0.1382363   time: 142.36\n",
      "===> Epoch 644 Complete: Avg. Loss: 0.1417768   time: 142.20\n",
      "===> Epoch 645 Complete: Avg. Loss: 0.1376543   time: 142.11\n",
      "===> Epoch 646 Complete: Avg. Loss: 0.1384398   time: 141.93\n",
      "===> Epoch 647 Complete: Avg. Loss: 0.1353449   time: 142.11\n",
      "===> Epoch 648 Complete: Avg. Loss: 0.1338223   time: 141.86\n",
      "===> Epoch 649 Complete: Avg. Loss: 0.1363458   time: 142.15\n",
      "===> Epoch 650 Complete: Avg. Loss: 0.1439735   time: 142.27\n",
      "===> Epoch 651 Complete: Avg. Loss: 0.1360530   time: 142.26\n",
      "===> Epoch 652 Complete: Avg. Loss: 0.1432948   time: 141.91\n",
      "===> Epoch 653 Complete: Avg. Loss: 0.1408029   time: 141.95\n",
      "===> Epoch 654 Complete: Avg. Loss: 0.1440901   time: 141.99\n",
      "===> Epoch 655 Complete: Avg. Loss: 0.1292116   time: 141.89\n",
      "===> Epoch 656 Complete: Avg. Loss: 0.1333695   time: 141.96\n",
      "===> Epoch 657 Complete: Avg. Loss: 0.1406492   time: 142.29\n",
      "===> Epoch 658 Complete: Avg. Loss: 0.1304414   time: 141.86\n",
      "===> Epoch 659 Complete: Avg. Loss: 0.1372363   time: 141.53\n",
      "===> Epoch 660 Complete: Avg. Loss: 0.1367531   time: 142.06\n",
      "===> Epoch 661 Complete: Avg. Loss: 0.1448158   time: 142.12\n",
      "===> Epoch 662 Complete: Avg. Loss: 0.1388777   time: 142.13\n",
      "===> Epoch 663 Complete: Avg. Loss: 0.1424269   time: 142.29\n",
      "===> Epoch 664 Complete: Avg. Loss: 0.1383483   time: 142.10\n",
      "===> Epoch 665 Complete: Avg. Loss: 0.1341437   time: 142.57\n",
      "===> Epoch 666 Complete: Avg. Loss: 0.1413241   time: 142.14\n",
      "===> Epoch 667 Complete: Avg. Loss: 0.1334104   time: 142.40\n",
      "===> Epoch 668 Complete: Avg. Loss: 0.1393433   time: 142.02\n",
      "===> Epoch 669 Complete: Avg. Loss: 0.1439095   time: 142.13\n",
      "===> Epoch 670 Complete: Avg. Loss: 0.1367406   time: 141.84\n",
      "===> Epoch 671 Complete: Avg. Loss: 0.1523045   time: 142.33\n",
      "===> Epoch 672 Complete: Avg. Loss: 0.1413821   time: 142.02\n",
      "===> Epoch 673 Complete: Avg. Loss: 0.1444993   time: 142.10\n",
      "===> Epoch 674 Complete: Avg. Loss: 0.1431103   time: 142.44\n",
      "===> Epoch 675 Complete: Avg. Loss: 0.1457467   time: 142.37\n",
      "===> Epoch 676 Complete: Avg. Loss: 0.1445313   time: 142.21\n",
      "===> Epoch 677 Complete: Avg. Loss: 0.1423015   time: 142.15\n",
      "===> Epoch 678 Complete: Avg. Loss: 0.1447498   time: 142.14\n",
      "===> Epoch 679 Complete: Avg. Loss: 0.1421354   time: 141.90\n",
      "===> Epoch 680 Complete: Avg. Loss: 0.1419576   time: 141.93\n",
      "===> Epoch 681 Complete: Avg. Loss: 0.1330308   time: 142.18\n",
      "===> Epoch 682 Complete: Avg. Loss: 0.1338578   time: 142.21\n",
      "===> Epoch 683 Complete: Avg. Loss: 0.1372879   time: 142.36\n",
      "===> Epoch 684 Complete: Avg. Loss: 0.1980589   time: 142.08\n",
      "===> Epoch 685 Complete: Avg. Loss: 0.1384355   time: 141.87\n",
      "===> Epoch 686 Complete: Avg. Loss: 0.1359335   time: 141.83\n",
      "===> Epoch 687 Complete: Avg. Loss: 0.1344983   time: 142.22\n",
      "===> Epoch 688 Complete: Avg. Loss: 0.1408580   time: 142.25\n",
      "===> Epoch 689 Complete: Avg. Loss: 0.1423530   time: 142.01\n",
      "===> Epoch 690 Complete: Avg. Loss: 0.1374566   time: 142.06\n",
      "===> Epoch 691 Complete: Avg. Loss: 0.1314245   time: 141.97\n",
      "===> Epoch 692 Complete: Avg. Loss: 0.1382153   time: 142.14\n",
      "===> Epoch 693 Complete: Avg. Loss: 0.1408764   time: 142.10\n",
      "===> Epoch 694 Complete: Avg. Loss: 0.1405045   time: 142.31\n",
      "===> Epoch 695 Complete: Avg. Loss: 0.1395153   time: 142.11\n",
      "===> Epoch 696 Complete: Avg. Loss: 0.1380777   time: 142.20\n",
      "===> Epoch 697 Complete: Avg. Loss: 0.1379764   time: 142.02\n",
      "===> Epoch 698 Complete: Avg. Loss: 0.1340720   time: 142.41\n",
      "===> Epoch 699 Complete: Avg. Loss: 0.1420111   time: 142.06\n",
      "===> Epoch 700 Complete: Avg. Loss: 0.1315384   time: 142.05\n",
      "[tensor([19.1600, 19.4980, 10.8066, 17.1689, 17.4198, 14.3987, 10.3160, 13.0090,\n",
      "        13.6839, 18.7218, 18.2228, 16.7289, 19.3871, 18.6625, 18.2957]), tensor([25.3727, 25.9443, 13.7800, 23.3723, 23.2483, 18.2583, 14.3142, 16.4494,\n",
      "        17.3235, 25.3983, 23.2074, 21.9740, 25.6248, 23.5236, 22.6558]), tensor([28.0824, 29.4711,  8.6736, 25.4479, 25.6468, 18.6525, 10.3022, 11.0555,\n",
      "        12.8996, 28.2745, 27.0090, 24.1107, 29.1740, 27.7829, 25.5750]), tensor([31.0471, 31.8496, 13.3527, 26.4463, 27.2928, 20.1460, 13.9903, 16.5118,\n",
      "        17.5694, 29.9586, 28.5552, 25.2103, 30.5194, 29.2836, 26.6949]), tensor([31.2505, 31.9779, 10.5622, 27.6869, 27.8998, 21.5530, 12.2783, 13.7427,\n",
      "        13.2072, 31.2582, 30.4114, 26.8475, 32.2056, 31.1638, 28.5424]), tensor([32.8996, 33.8295, 10.5872, 28.9906, 28.7111, 23.1501, 12.0393, 13.1413,\n",
      "        13.4252, 32.7423, 31.8044, 27.5045, 33.4359, 32.6657, 29.0726]), tensor([32.9366, 33.1678,  9.8976, 29.0594, 28.9095, 21.7044, 10.3222, 10.7021,\n",
      "        11.1292, 33.2589, 31.8781, 28.1193, 33.8123, 32.5355, 29.6833]), tensor([32.9396, 34.2519,  6.6674, 29.5173, 29.4873, 23.5483,  8.0796,  6.9038,\n",
      "         6.4501, 33.7141, 32.4691, 28.5605, 34.8731, 33.5509, 30.3518])]\n",
      "Test result: 24.7577\n",
      "Checkpoint saved to ./model/25_02_09_16_45_Recons_depth_4x(pretrain)/S9_model_epoch_700.pth\n",
      "===> Epoch 701 Complete: Avg. Loss: 0.1474940   time: 142.21\n",
      "===> Epoch 702 Complete: Avg. Loss: 0.1364803   time: 142.14\n",
      "===> Epoch 703 Complete: Avg. Loss: 0.1413591   time: 142.05\n",
      "===> Epoch 704 Complete: Avg. Loss: 0.1358956   time: 142.26\n",
      "===> Epoch 705 Complete: Avg. Loss: 0.1362762   time: 142.18\n",
      "===> Epoch 706 Complete: Avg. Loss: 0.1414307   time: 142.01\n",
      "===> Epoch 707 Complete: Avg. Loss: 0.1371763   time: 142.07\n",
      "===> Epoch 708 Complete: Avg. Loss: 0.1382070   time: 141.91\n",
      "===> Epoch 709 Complete: Avg. Loss: 0.1364239   time: 142.17\n",
      "===> Epoch 710 Complete: Avg. Loss: 0.1365741   time: 142.10\n",
      "===> Epoch 711 Complete: Avg. Loss: 0.1405137   time: 142.20\n",
      "===> Epoch 712 Complete: Avg. Loss: 0.1438484   time: 142.11\n",
      "===> Epoch 713 Complete: Avg. Loss: 0.1447758   time: 142.30\n",
      "===> Epoch 714 Complete: Avg. Loss: 0.1360736   time: 141.87\n",
      "===> Epoch 715 Complete: Avg. Loss: 0.1339030   time: 142.24\n",
      "===> Epoch 716 Complete: Avg. Loss: 0.1428758   time: 142.09\n",
      "===> Epoch 717 Complete: Avg. Loss: 0.1355828   time: 142.14\n",
      "===> Epoch 718 Complete: Avg. Loss: 0.1381894   time: 141.79\n",
      "===> Epoch 719 Complete: Avg. Loss: 0.1514894   time: 142.17\n",
      "===> Epoch 720 Complete: Avg. Loss: 0.1505910   time: 141.67\n",
      "===> Epoch 721 Complete: Avg. Loss: 0.1359446   time: 141.92\n",
      "===> Epoch 722 Complete: Avg. Loss: 0.1373383   time: 141.87\n",
      "===> Epoch 723 Complete: Avg. Loss: 0.1403643   time: 142.55\n",
      "===> Epoch 724 Complete: Avg. Loss: 0.1405936   time: 143.32\n",
      "===> Epoch 725 Complete: Avg. Loss: 0.1403232   time: 142.72\n",
      "===> Epoch 726 Complete: Avg. Loss: 0.1393892   time: 143.22\n",
      "===> Epoch 727 Complete: Avg. Loss: 0.1359363   time: 142.48\n",
      "===> Epoch 728 Complete: Avg. Loss: 0.1291874   time: 141.91\n",
      "===> Epoch 729 Complete: Avg. Loss: 0.1342498   time: 142.35\n",
      "===> Epoch 730 Complete: Avg. Loss: 0.1395900   time: 141.92\n",
      "===> Epoch 731 Complete: Avg. Loss: 0.1346389   time: 141.95\n",
      "===> Epoch 732 Complete: Avg. Loss: 0.1401678   time: 142.15\n",
      "===> Epoch 733 Complete: Avg. Loss: 0.1399507   time: 141.94\n",
      "===> Epoch 734 Complete: Avg. Loss: 0.1320930   time: 142.11\n",
      "===> Epoch 735 Complete: Avg. Loss: 0.1328377   time: 142.27\n",
      "===> Epoch 736 Complete: Avg. Loss: 0.1347812   time: 141.83\n",
      "===> Epoch 737 Complete: Avg. Loss: 0.1361807   time: 142.02\n",
      "===> Epoch 738 Complete: Avg. Loss: 0.1357769   time: 142.29\n",
      "===> Epoch 739 Complete: Avg. Loss: 0.1308435   time: 142.29\n",
      "===> Epoch 740 Complete: Avg. Loss: 0.1356207   time: 142.00\n",
      "===> Epoch 741 Complete: Avg. Loss: 0.1368427   time: 142.12\n",
      "===> Epoch 742 Complete: Avg. Loss: 0.1393374   time: 142.32\n",
      "===> Epoch 743 Complete: Avg. Loss: 0.1378706   time: 142.28\n",
      "===> Epoch 744 Complete: Avg. Loss: 0.1383383   time: 142.03\n",
      "===> Epoch 745 Complete: Avg. Loss: 0.1369403   time: 141.94\n",
      "===> Epoch 746 Complete: Avg. Loss: 0.1375966   time: 142.30\n",
      "===> Epoch 747 Complete: Avg. Loss: 0.1395700   time: 142.10\n",
      "===> Epoch 748 Complete: Avg. Loss: 0.1857039   time: 141.94\n",
      "===> Epoch 749 Complete: Avg. Loss: 0.1304939   time: 142.01\n",
      "===> Epoch 750 Complete: Avg. Loss: 0.1375551   time: 141.89\n",
      "===> Epoch 751 Complete: Avg. Loss: 0.1344087   time: 142.20\n",
      "===> Epoch 752 Complete: Avg. Loss: 0.1427138   time: 142.25\n",
      "===> Epoch 753 Complete: Avg. Loss: 0.1314765   time: 142.18\n",
      "===> Epoch 754 Complete: Avg. Loss: 0.1317454   time: 142.07\n",
      "===> Epoch 755 Complete: Avg. Loss: 0.1356797   time: 142.01\n",
      "===> Epoch 756 Complete: Avg. Loss: 0.1356943   time: 142.09\n",
      "===> Epoch 757 Complete: Avg. Loss: 0.1356763   time: 141.88\n",
      "===> Epoch 758 Complete: Avg. Loss: 0.1390280   time: 141.87\n",
      "===> Epoch 759 Complete: Avg. Loss: 0.1372315   time: 142.47\n",
      "===> Epoch 760 Complete: Avg. Loss: 0.1251151   time: 142.20\n",
      "===> Epoch 761 Complete: Avg. Loss: 0.1381380   time: 141.94\n",
      "===> Epoch 762 Complete: Avg. Loss: 0.1342234   time: 142.06\n",
      "===> Epoch 763 Complete: Avg. Loss: 0.1306146   time: 142.14\n",
      "===> Epoch 764 Complete: Avg. Loss: 0.1366884   time: 142.12\n",
      "===> Epoch 765 Complete: Avg. Loss: 0.1332413   time: 142.26\n",
      "===> Epoch 766 Complete: Avg. Loss: 0.1299899   time: 141.76\n",
      "===> Epoch 767 Complete: Avg. Loss: 0.1341131   time: 142.29\n",
      "===> Epoch 768 Complete: Avg. Loss: 0.1368329   time: 142.14\n",
      "===> Epoch 769 Complete: Avg. Loss: 0.1306453   time: 142.16\n",
      "===> Epoch 770 Complete: Avg. Loss: 0.1259046   time: 142.10\n",
      "===> Epoch 771 Complete: Avg. Loss: 0.1397868   time: 142.17\n",
      "===> Epoch 772 Complete: Avg. Loss: 0.1917416   time: 142.89\n",
      "===> Epoch 773 Complete: Avg. Loss: 0.1319277   time: 141.99\n",
      "===> Epoch 774 Complete: Avg. Loss: 0.1335832   time: 142.17\n",
      "===> Epoch 775 Complete: Avg. Loss: 0.1376222   time: 141.89\n",
      "===> Epoch 776 Complete: Avg. Loss: 0.1356094   time: 141.96\n",
      "===> Epoch 777 Complete: Avg. Loss: 0.1376811   time: 141.90\n",
      "===> Epoch 778 Complete: Avg. Loss: 0.1318614   time: 142.35\n",
      "===> Epoch 779 Complete: Avg. Loss: 0.1358500   time: 142.11\n",
      "===> Epoch 780 Complete: Avg. Loss: 0.1379673   time: 142.12\n",
      "===> Epoch 781 Complete: Avg. Loss: 0.1332385   time: 142.24\n",
      "===> Epoch 782 Complete: Avg. Loss: 0.1325187   time: 141.97\n",
      "===> Epoch 783 Complete: Avg. Loss: 0.1358339   time: 141.97\n",
      "===> Epoch 784 Complete: Avg. Loss: 0.1297277   time: 141.97\n",
      "===> Epoch 785 Complete: Avg. Loss: 0.1313489   time: 142.40\n",
      "===> Epoch 786 Complete: Avg. Loss: 0.1286136   time: 142.00\n",
      "===> Epoch 787 Complete: Avg. Loss: 0.1344929   time: 142.05\n",
      "===> Epoch 788 Complete: Avg. Loss: 0.1288715   time: 142.32\n",
      "===> Epoch 789 Complete: Avg. Loss: 0.1289312   time: 142.00\n",
      "===> Epoch 790 Complete: Avg. Loss: 0.1334125   time: 142.02\n",
      "===> Epoch 791 Complete: Avg. Loss: 0.1341642   time: 141.86\n",
      "===> Epoch 792 Complete: Avg. Loss: 0.1291166   time: 141.95\n",
      "===> Epoch 793 Complete: Avg. Loss: 0.1302548   time: 141.88\n",
      "===> Epoch 794 Complete: Avg. Loss: 0.1310974   time: 142.12\n",
      "===> Epoch 795 Complete: Avg. Loss: 0.1438307   time: 142.11\n",
      "===> Epoch 796 Complete: Avg. Loss: 0.1351076   time: 141.68\n",
      "===> Epoch 797 Complete: Avg. Loss: 0.1359869   time: 142.16\n",
      "===> Epoch 798 Complete: Avg. Loss: 0.1449638   time: 141.86\n",
      "===> Epoch 799 Complete: Avg. Loss: 0.1434682   time: 141.96\n",
      "===> Epoch 800 Complete: Avg. Loss: 0.1330734   time: 141.98\n",
      "[tensor([19.1600, 19.4980, 10.8066, 17.1689, 17.4198, 14.3987, 10.3160, 13.0090,\n",
      "        13.6839, 18.7218, 18.2228, 16.7289, 19.3871, 18.6625, 18.2957]), tensor([25.3727, 25.9443, 13.7800, 23.3723, 23.2483, 18.2583, 14.3142, 16.4494,\n",
      "        17.3235, 25.3983, 23.2074, 21.9740, 25.6248, 23.5236, 22.6558]), tensor([28.0824, 29.4711,  8.6736, 25.4479, 25.6468, 18.6525, 10.3022, 11.0555,\n",
      "        12.8996, 28.2745, 27.0090, 24.1107, 29.1740, 27.7829, 25.5750]), tensor([31.0471, 31.8496, 13.3527, 26.4463, 27.2928, 20.1460, 13.9903, 16.5118,\n",
      "        17.5694, 29.9586, 28.5552, 25.2103, 30.5194, 29.2836, 26.6949]), tensor([31.2505, 31.9779, 10.5622, 27.6869, 27.8998, 21.5530, 12.2783, 13.7427,\n",
      "        13.2072, 31.2582, 30.4114, 26.8475, 32.2056, 31.1638, 28.5424]), tensor([32.8996, 33.8295, 10.5872, 28.9906, 28.7111, 23.1501, 12.0393, 13.1413,\n",
      "        13.4252, 32.7423, 31.8044, 27.5045, 33.4359, 32.6657, 29.0726]), tensor([32.9366, 33.1678,  9.8976, 29.0594, 28.9095, 21.7044, 10.3222, 10.7021,\n",
      "        11.1292, 33.2589, 31.8781, 28.1193, 33.8123, 32.5355, 29.6833]), tensor([32.9396, 34.2519,  6.6674, 29.5173, 29.4873, 23.5483,  8.0796,  6.9038,\n",
      "         6.4501, 33.7141, 32.4691, 28.5605, 34.8731, 33.5509, 30.3518]), tensor([33.1123, 33.7585, 11.4824, 29.4716, 29.2911, 23.1335, 13.9772, 15.7922,\n",
      "        15.4481, 33.6947, 32.6050, 28.3118, 34.8786, 33.6418, 30.2249])]\n",
      "Test result: 26.5882\n",
      "Checkpoint saved to ./model/25_02_09_16_45_Recons_depth_4x(pretrain)/S9_model_epoch_800.pth\n",
      "===> Epoch 801 Complete: Avg. Loss: 0.1355083   time: 142.15\n",
      "===> Epoch 802 Complete: Avg. Loss: 0.1315854   time: 142.08\n",
      "===> Epoch 803 Complete: Avg. Loss: 0.1259477   time: 141.89\n",
      "===> Epoch 804 Complete: Avg. Loss: 0.1379742   time: 141.87\n",
      "===> Epoch 805 Complete: Avg. Loss: 0.1311022   time: 141.90\n",
      "===> Epoch 806 Complete: Avg. Loss: 0.1405468   time: 142.12\n",
      "===> Epoch 807 Complete: Avg. Loss: 0.1342466   time: 141.99\n",
      "===> Epoch 808 Complete: Avg. Loss: 0.1258026   time: 141.93\n",
      "===> Epoch 809 Complete: Avg. Loss: 0.1325761   time: 142.07\n",
      "===> Epoch 810 Complete: Avg. Loss: 0.1342491   time: 142.15\n",
      "===> Epoch 811 Complete: Avg. Loss: 0.1310569   time: 142.32\n",
      "===> Epoch 812 Complete: Avg. Loss: 0.1329574   time: 141.86\n",
      "===> Epoch 813 Complete: Avg. Loss: 0.1306307   time: 141.66\n",
      "===> Epoch 814 Complete: Avg. Loss: 0.1361314   time: 141.92\n",
      "===> Epoch 815 Complete: Avg. Loss: 0.1286255   time: 141.90\n",
      "===> Epoch 816 Complete: Avg. Loss: 0.1345099   time: 142.18\n",
      "===> Epoch 817 Complete: Avg. Loss: 0.1298190   time: 142.06\n",
      "===> Epoch 818 Complete: Avg. Loss: 0.1310709   time: 141.83\n",
      "===> Epoch 819 Complete: Avg. Loss: 0.1334312   time: 141.89\n",
      "===> Epoch 820 Complete: Avg. Loss: 0.1314300   time: 142.01\n",
      "===> Epoch 821 Complete: Avg. Loss: 0.1235684   time: 141.71\n",
      "===> Epoch 822 Complete: Avg. Loss: 0.1323654   time: 141.82\n",
      "===> Epoch 823 Complete: Avg. Loss: 0.1364497   time: 142.48\n",
      "===> Epoch 824 Complete: Avg. Loss: 0.1420319   time: 142.19\n",
      "===> Epoch 825 Complete: Avg. Loss: 0.1359060   time: 141.74\n",
      "===> Epoch 826 Complete: Avg. Loss: 0.1320726   time: 141.94\n",
      "===> Epoch 827 Complete: Avg. Loss: 0.1258450   time: 141.86\n",
      "===> Epoch 828 Complete: Avg. Loss: 0.1464562   time: 142.26\n",
      "===> Epoch 829 Complete: Avg. Loss: 0.1424875   time: 142.17\n",
      "===> Epoch 830 Complete: Avg. Loss: 0.1467301   time: 141.95\n",
      "===> Epoch 831 Complete: Avg. Loss: 0.1358439   time: 141.92\n",
      "===> Epoch 832 Complete: Avg. Loss: 0.1303247   time: 141.92\n",
      "===> Epoch 833 Complete: Avg. Loss: 0.1323042   time: 142.06\n",
      "===> Epoch 834 Complete: Avg. Loss: 0.1299404   time: 141.86\n",
      "===> Epoch 835 Complete: Avg. Loss: 0.1307520   time: 142.07\n",
      "===> Epoch 836 Complete: Avg. Loss: 0.1321567   time: 141.84\n",
      "===> Epoch 837 Complete: Avg. Loss: 0.1310565   time: 141.48\n",
      "===> Epoch 838 Complete: Avg. Loss: 0.1332217   time: 142.11\n",
      "===> Epoch 839 Complete: Avg. Loss: 0.1371851   time: 141.98\n",
      "===> Epoch 840 Complete: Avg. Loss: 0.1348561   time: 141.99\n",
      "===> Epoch 841 Complete: Avg. Loss: 0.1350382   time: 142.03\n",
      "===> Epoch 842 Complete: Avg. Loss: 0.1358683   time: 142.11\n",
      "===> Epoch 843 Complete: Avg. Loss: 0.1408596   time: 141.77\n",
      "===> Epoch 844 Complete: Avg. Loss: 0.1336350   time: 142.37\n",
      "===> Epoch 845 Complete: Avg. Loss: 0.1402172   time: 142.42\n",
      "===> Epoch 846 Complete: Avg. Loss: 0.1350371   time: 142.34\n",
      "===> Epoch 847 Complete: Avg. Loss: 0.1269773   time: 142.24\n",
      "===> Epoch 848 Complete: Avg. Loss: 0.1334552   time: 142.10\n",
      "===> Epoch 849 Complete: Avg. Loss: 0.1406328   time: 141.87\n",
      "===> Epoch 850 Complete: Avg. Loss: 0.1299479   time: 141.73\n",
      "===> Epoch 851 Complete: Avg. Loss: 0.1331624   time: 141.60\n",
      "===> Epoch 852 Complete: Avg. Loss: 0.1284095   time: 141.90\n",
      "===> Epoch 853 Complete: Avg. Loss: 0.1293929   time: 142.31\n",
      "===> Epoch 854 Complete: Avg. Loss: 0.1343845   time: 142.10\n",
      "===> Epoch 855 Complete: Avg. Loss: 0.1386394   time: 141.92\n",
      "===> Epoch 856 Complete: Avg. Loss: 0.1313556   time: 141.94\n",
      "===> Epoch 857 Complete: Avg. Loss: 0.1336168   time: 141.78\n",
      "===> Epoch 858 Complete: Avg. Loss: 0.1367213   time: 141.77\n",
      "===> Epoch 859 Complete: Avg. Loss: 0.1347800   time: 142.17\n",
      "===> Epoch 860 Complete: Avg. Loss: 0.1401157   time: 141.80\n",
      "===> Epoch 861 Complete: Avg. Loss: 0.1337853   time: 142.10\n",
      "===> Epoch 862 Complete: Avg. Loss: 0.1299872   time: 142.12\n",
      "===> Epoch 863 Complete: Avg. Loss: 0.1268305   time: 141.67\n",
      "===> Epoch 864 Complete: Avg. Loss: 0.1275980   time: 141.90\n",
      "===> Epoch 865 Complete: Avg. Loss: 0.1280340   time: 141.76\n",
      "===> Epoch 866 Complete: Avg. Loss: 0.1857544   time: 142.09\n",
      "===> Epoch 867 Complete: Avg. Loss: 0.1452980   time: 141.71\n",
      "===> Epoch 868 Complete: Avg. Loss: 0.1316642   time: 142.11\n",
      "===> Epoch 869 Complete: Avg. Loss: 0.1345417   time: 141.79\n",
      "===> Epoch 870 Complete: Avg. Loss: 0.1276055   time: 142.21\n",
      "===> Epoch 871 Complete: Avg. Loss: 0.1277406   time: 142.04\n",
      "===> Epoch 872 Complete: Avg. Loss: 0.1304543   time: 141.74\n",
      "===> Epoch 873 Complete: Avg. Loss: 0.1318066   time: 142.14\n",
      "===> Epoch 874 Complete: Avg. Loss: 0.1286357   time: 141.79\n",
      "===> Epoch 875 Complete: Avg. Loss: 0.1323168   time: 141.97\n",
      "===> Epoch 876 Complete: Avg. Loss: 0.1368948   time: 142.62\n",
      "===> Epoch 877 Complete: Avg. Loss: 0.1306600   time: 141.96\n",
      "===> Epoch 878 Complete: Avg. Loss: 0.1287612   time: 142.52\n",
      "===> Epoch 879 Complete: Avg. Loss: 0.1271011   time: 142.27\n",
      "===> Epoch 880 Complete: Avg. Loss: 0.1869424   time: 142.33\n",
      "===> Epoch 881 Complete: Avg. Loss: 0.1230790   time: 142.15\n",
      "===> Epoch 882 Complete: Avg. Loss: 0.1330719   time: 142.16\n",
      "===> Epoch 883 Complete: Avg. Loss: 0.1875401   time: 142.11\n",
      "===> Epoch 884 Complete: Avg. Loss: 0.1308531   time: 141.99\n",
      "===> Epoch 885 Complete: Avg. Loss: 0.1412918   time: 142.32\n",
      "===> Epoch 886 Complete: Avg. Loss: 0.1301163   time: 142.17\n",
      "===> Epoch 887 Complete: Avg. Loss: 0.1318368   time: 142.15\n",
      "===> Epoch 888 Complete: Avg. Loss: 0.1266498   time: 142.58\n",
      "===> Epoch 889 Complete: Avg. Loss: 0.1305375   time: 142.01\n",
      "===> Epoch 890 Complete: Avg. Loss: 0.1349471   time: 142.06\n",
      "===> Epoch 891 Complete: Avg. Loss: 0.1294142   time: 142.04\n",
      "===> Epoch 892 Complete: Avg. Loss: 0.1263819   time: 142.19\n",
      "===> Epoch 893 Complete: Avg. Loss: 0.1275877   time: 142.09\n",
      "===> Epoch 894 Complete: Avg. Loss: 0.1289092   time: 142.20\n",
      "===> Epoch 895 Complete: Avg. Loss: 0.1295820   time: 141.82\n",
      "===> Epoch 896 Complete: Avg. Loss: 0.1305706   time: 142.45\n",
      "===> Epoch 897 Complete: Avg. Loss: 0.1278403   time: 142.14\n",
      "===> Epoch 898 Complete: Avg. Loss: 0.1320955   time: 142.03\n",
      "===> Epoch 899 Complete: Avg. Loss: 0.1331890   time: 142.08\n",
      "===> Epoch 900 Complete: Avg. Loss: 0.1340481   time: 142.20\n",
      "[tensor([19.1600, 19.4980, 10.8066, 17.1689, 17.4198, 14.3987, 10.3160, 13.0090,\n",
      "        13.6839, 18.7218, 18.2228, 16.7289, 19.3871, 18.6625, 18.2957]), tensor([25.3727, 25.9443, 13.7800, 23.3723, 23.2483, 18.2583, 14.3142, 16.4494,\n",
      "        17.3235, 25.3983, 23.2074, 21.9740, 25.6248, 23.5236, 22.6558]), tensor([28.0824, 29.4711,  8.6736, 25.4479, 25.6468, 18.6525, 10.3022, 11.0555,\n",
      "        12.8996, 28.2745, 27.0090, 24.1107, 29.1740, 27.7829, 25.5750]), tensor([31.0471, 31.8496, 13.3527, 26.4463, 27.2928, 20.1460, 13.9903, 16.5118,\n",
      "        17.5694, 29.9586, 28.5552, 25.2103, 30.5194, 29.2836, 26.6949]), tensor([31.2505, 31.9779, 10.5622, 27.6869, 27.8998, 21.5530, 12.2783, 13.7427,\n",
      "        13.2072, 31.2582, 30.4114, 26.8475, 32.2056, 31.1638, 28.5424]), tensor([32.8996, 33.8295, 10.5872, 28.9906, 28.7111, 23.1501, 12.0393, 13.1413,\n",
      "        13.4252, 32.7423, 31.8044, 27.5045, 33.4359, 32.6657, 29.0726]), tensor([32.9366, 33.1678,  9.8976, 29.0594, 28.9095, 21.7044, 10.3222, 10.7021,\n",
      "        11.1292, 33.2589, 31.8781, 28.1193, 33.8123, 32.5355, 29.6833]), tensor([32.9396, 34.2519,  6.6674, 29.5173, 29.4873, 23.5483,  8.0796,  6.9038,\n",
      "         6.4501, 33.7141, 32.4691, 28.5605, 34.8731, 33.5509, 30.3518]), tensor([33.1123, 33.7585, 11.4824, 29.4716, 29.2911, 23.1335, 13.9772, 15.7922,\n",
      "        15.4481, 33.6947, 32.6050, 28.3118, 34.8786, 33.6418, 30.2249]), tensor([33.2638, 33.6204, 11.5280, 29.3190, 28.9435, 23.3317, 14.5849, 15.6976,\n",
      "        14.9017, 33.7823, 32.7686, 28.3851, 34.9096, 33.6100, 30.2439])]\n",
      "Test result: 26.5927\n",
      "Checkpoint saved to ./model/25_02_09_16_45_Recons_depth_4x(pretrain)/S9_model_epoch_900.pth\n",
      "===> Epoch 901 Complete: Avg. Loss: 0.1283797   time: 142.03\n",
      "===> Epoch 902 Complete: Avg. Loss: 0.1302677   time: 141.99\n",
      "===> Epoch 903 Complete: Avg. Loss: 0.1295326   time: 141.94\n",
      "===> Epoch 904 Complete: Avg. Loss: 0.1287456   time: 142.26\n",
      "===> Epoch 905 Complete: Avg. Loss: 0.1258393   time: 141.72\n",
      "===> Epoch 906 Complete: Avg. Loss: 0.1291206   time: 142.12\n",
      "===> Epoch 907 Complete: Avg. Loss: 0.1344244   time: 141.99\n",
      "===> Epoch 908 Complete: Avg. Loss: 0.1283409   time: 142.08\n",
      "===> Epoch 909 Complete: Avg. Loss: 0.1400999   time: 142.35\n",
      "===> Epoch 910 Complete: Avg. Loss: 0.1258273   time: 142.02\n",
      "===> Epoch 911 Complete: Avg. Loss: 0.1360006   time: 142.32\n",
      "===> Epoch 912 Complete: Avg. Loss: 0.1272009   time: 142.18\n",
      "===> Epoch 913 Complete: Avg. Loss: 0.1332102   time: 142.57\n",
      "===> Epoch 914 Complete: Avg. Loss: 0.1257114   time: 142.30\n",
      "===> Epoch 915 Complete: Avg. Loss: 0.1845952   time: 142.38\n",
      "===> Epoch 916 Complete: Avg. Loss: 0.1264031   time: 141.85\n",
      "===> Epoch 917 Complete: Avg. Loss: 0.1263210   time: 141.90\n",
      "===> Epoch 918 Complete: Avg. Loss: 0.1347214   time: 142.32\n",
      "===> Epoch 919 Complete: Avg. Loss: 0.1307831   time: 142.25\n",
      "===> Epoch 920 Complete: Avg. Loss: 0.1293941   time: 142.28\n",
      "===> Epoch 921 Complete: Avg. Loss: 0.1253376   time: 142.30\n",
      "===> Epoch 922 Complete: Avg. Loss: 0.1316970   time: 142.04\n",
      "===> Epoch 923 Complete: Avg. Loss: 0.1373701   time: 141.87\n",
      "===> Epoch 924 Complete: Avg. Loss: 0.1328298   time: 142.30\n",
      "===> Epoch 925 Complete: Avg. Loss: 0.1275280   time: 142.15\n",
      "===> Epoch 926 Complete: Avg. Loss: 0.1251743   time: 142.27\n",
      "===> Epoch 927 Complete: Avg. Loss: 0.1350574   time: 142.19\n",
      "===> Epoch 928 Complete: Avg. Loss: 0.1333575   time: 142.52\n",
      "===> Epoch 929 Complete: Avg. Loss: 0.1343077   time: 142.01\n",
      "===> Epoch 930 Complete: Avg. Loss: 0.1337751   time: 142.00\n",
      "===> Epoch 931 Complete: Avg. Loss: 0.1284639   time: 142.08\n",
      "===> Epoch 932 Complete: Avg. Loss: 0.1359620   time: 142.04\n",
      "===> Epoch 933 Complete: Avg. Loss: 0.1312629   time: 142.03\n",
      "===> Epoch 934 Complete: Avg. Loss: 0.1327870   time: 141.98\n",
      "===> Epoch 935 Complete: Avg. Loss: 0.1362607   time: 141.99\n",
      "===> Epoch 936 Complete: Avg. Loss: 0.1452344   time: 142.09\n",
      "===> Epoch 937 Complete: Avg. Loss: 0.1869236   time: 142.46\n",
      "===> Epoch 938 Complete: Avg. Loss: 0.1287900   time: 142.07\n",
      "===> Epoch 939 Complete: Avg. Loss: 0.1289712   time: 142.16\n",
      "===> Epoch 940 Complete: Avg. Loss: 0.1809085   time: 142.18\n",
      "===> Epoch 941 Complete: Avg. Loss: 0.1290101   time: 142.12\n",
      "===> Epoch 942 Complete: Avg. Loss: 0.1321482   time: 142.33\n",
      "===> Epoch 943 Complete: Avg. Loss: 0.1296248   time: 142.15\n",
      "===> Epoch 944 Complete: Avg. Loss: 0.1205286   time: 142.14\n",
      "===> Epoch 945 Complete: Avg. Loss: 0.1398432   time: 142.18\n",
      "===> Epoch 946 Complete: Avg. Loss: 0.1379072   time: 142.20\n",
      "===> Epoch 947 Complete: Avg. Loss: 0.1272529   time: 142.23\n",
      "===> Epoch 948 Complete: Avg. Loss: 0.1301693   time: 142.19\n",
      "===> Epoch 949 Complete: Avg. Loss: 0.1252152   time: 141.95\n",
      "===> Epoch 950 Complete: Avg. Loss: 0.1339441   time: 142.06\n",
      "===> Epoch 951 Complete: Avg. Loss: 0.1352315   time: 142.39\n",
      "===> Epoch 952 Complete: Avg. Loss: 0.1335582   time: 141.83\n",
      "===> Epoch 953 Complete: Avg. Loss: 0.1301190   time: 142.26\n",
      "===> Epoch 954 Complete: Avg. Loss: 0.1376888   time: 142.09\n",
      "===> Epoch 955 Complete: Avg. Loss: 0.1272452   time: 142.20\n",
      "===> Epoch 956 Complete: Avg. Loss: 0.1301368   time: 142.04\n",
      "===> Epoch 957 Complete: Avg. Loss: 0.1277783   time: 141.87\n",
      "===> Epoch 958 Complete: Avg. Loss: 0.1328907   time: 142.01\n",
      "===> Epoch 959 Complete: Avg. Loss: 0.1397419   time: 142.50\n"
     ]
    }
   ],
   "source": [
    "# RUN training\n",
    "##############\n",
    "\n",
    "# network = ADMM_128().cuda()\n",
    "network = ADMM_depthnet128().cuda()\n",
    "\n",
    "if last_epoch != 0:\n",
    "    network = torch.load(\n",
    "        './model/' + model_save_filename + \"/S{}_model_epoch_{}.pth\".format(stage_num, last_epoch))\n",
    "    \n",
    "if multi_path:\n",
    "    dataset = Imgdataset_multipath(data_path)\n",
    "else:\n",
    "    dataset = Imgdataset(data_path)\n",
    "\n",
    "train_data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "mask_check_data_loader = DataLoader(dataset=dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "model_name = 'Recons_depth_4x(pretrain)'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(learning_rate,depth_learning_rate,model_name,mask, mask_r, mask_s,noise_add,separate,loss_alpha,loss_beta_i,loss_beta_d,multi_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b41f165-a4d3-41ab-aee6-9e1ca25402f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "NVIDIA RTX A5000\n",
      "25_02_09_16_45_Recons_depth_4x(pretrain)\n",
      "['1215_corner_wo.mat']\n",
      "0\n",
      "./real/1215_corner_wo.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhayashi\\AppData\\Local\\Temp\\ipykernel_24956\\2334936273.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  network.load_state_dict(torch.load(r'./model/' + model_name + '/S9_model_epoch_600_state_dict.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7010691165924072 sec.\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "# Reproduction for real \n",
    "#######################\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Check the avtive DNN\n",
    "######################\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "GPU_number =2 # if you wanna use A5000, 2\n",
    "torch.cuda.set_device(GPU_number)\n",
    "print(torch.cuda.get_device_name())\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception('NO GPU!')\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception('NO GPU!')\n",
    "\n",
    "# Choose Network\n",
    "###################\n",
    "a = ['25_02_09_16_45_Recons_depth_4x(pretrain)']\n",
    "# network = ADMM_128().cuda()\n",
    "network = ADMM_depthnet128().cuda()\n",
    "\n",
    "# real scene test path\n",
    "######################\n",
    "calibration = 40000\n",
    "test_path = r\"./real\"\n",
    "mask_path = r\"./matlab\"\n",
    "init_mask = 'real_opt_4x'\n",
    "frame_num = 128 #the lenght of transient images.\n",
    "shutter_bit = frame_num\n",
    "mask, mask_r, mask_s = generate_masks2(mask_path,shutter_bit,init_mask)\n",
    "Phi_r = mask_r.repeat([1, 1,1, 94, 106])\n",
    "Phi = mask.repeat([1,1,1])\n",
    "Phi_s = torch.sum(Phi_r,dim=1)\n",
    "    \n",
    "for model_name in a:\n",
    "    print(model_name)\n",
    "    result_path = 'recon/' + model_name\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    psnr_epoch = []\n",
    "    test_list = [f for f in os.listdir(test_path) if f.endswith('.mat')]\n",
    "    print(test_list)\n",
    "    psnr_sample = torch.zeros(len(test_list))\n",
    "    pred = []\n",
    "    outdepth = []\n",
    "    for i_test in range(0,len(test_list)):\n",
    "        print(i_test)\n",
    "        print(test_path + '/' + test_list[i_test])\n",
    "        gt = scio.loadmat(test_path + '/' + test_list[i_test])\n",
    "        gt = gt['Gresult']\n",
    "        calibration = 1.5*np.max(gt)\n",
    "        gt = torch.from_numpy(np.array(gt))\n",
    "        gt = torch.unsqueeze(gt.permute(2,0,1),0).float().cuda()/calibration#c,h,w\n",
    "        y = gt\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            network.load_state_dict(torch.load(r'./model/' + model_name + '/S9_model_epoch_600_state_dict.pth')) # The best epoch iterations is 600 for real scene i think. \n",
    "            out_pic_list = network(y, Phi, Phi_r, Phi_s)\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed_time = time.time() - start\n",
    "            print(elapsed_time, 'sec.')\n",
    "            \n",
    "            out_pic = out_pic_list[-4] #K > 3\n",
    "            out_depth = out_pic_list[-1] \n",
    "            pred.append(out_pic.cpu().numpy())\n",
    "            outdepth.append(out_depth.cpu().numpy())\n",
    "            name = result_path + '/real_{}'.format(int(calibration))+'reproduced_'+ test_list[i_test] \n",
    "            scio.savemat(name, {'pred': pred})\n",
    "            name2 = result_path + '/real_{}'.format(int(calibration))+'depth_' + test_list[i_test]\n",
    "            scio.savemat(name2, {'outdepth': outdepth})\n",
    "            pred = []\n",
    "            outdepth = []\n",
    "    print('b')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f89a05-d248-4e8c-8031-d7ca9d9b3820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
